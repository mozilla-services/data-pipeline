{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Socorro crash data into the Data Platform\n",
    "\n",
    "We want to be able to store Socorro crash data in Parquet form so that it can be made accessible from re:dash.\n",
    "\n",
    "See [Bug 1273657](https://bugzilla.mozilla.org/show_bug.cgi?id=1273657) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the pyspark datatype for representing the crash data in spark. This is a slightly modified version of [peterbe/crash-report-struct-code](https://github.com/peterbe/crash-report-struct-code).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def create_struct(schema):\n",
    "    \"\"\" Take a JSON schema and return a pyspark StructType of equivalent structure. \"\"\"\n",
    "    \n",
    "    replace_definitions(schema, schema['definitions'])\n",
    "    assert '$ref' not in str(schema), 're-write didnt work'\n",
    "    \n",
    "    struct = StructType()\n",
    "    for row in get_rows(schema):\n",
    "        struct.add(row)\n",
    "\n",
    "    return struct\n",
    "\n",
    "def replace_definitions(schema, definitions):\n",
    "    \"\"\" Replace references in the JSON schema with their definitions.\"\"\"\n",
    "\n",
    "    if 'properties' in schema:\n",
    "        for prop, meta in schema['properties'].items():\n",
    "            replace_definitions(meta, definitions)\n",
    "    elif 'items' in schema:\n",
    "        if '$ref' in schema['items']:\n",
    "            ref = schema['items']['$ref'].split('/')[-1]\n",
    "            schema['items'] = definitions[ref]\n",
    "            replace_definitions(schema['items'], definitions)\n",
    "        else:\n",
    "            replace_definitions(schema['items'], definitions)\n",
    "    elif '$ref' in str(schema):\n",
    "        log.error(\"Reference not found for schema: {}\".format(str(schema)))\n",
    "        raise Exception\n",
    "\n",
    "def get_rows(schema):\n",
    "    \"\"\" Map the fields in a JSON schema to corresponding data structures in pyspark.\"\"\"\n",
    "    \n",
    "    if 'properties' not in schema:\n",
    "        log.error(\"Invalid JSON schema: properties field is missing.\")\n",
    "        raise Exception\n",
    "        \n",
    "    for prop in sorted(schema['properties']):\n",
    "        meta = schema['properties'][prop]\n",
    "        if 'string' in meta['type']:\n",
    "            logging.debug(\"{!r} allows the type to be String AND Integer\".format(prop))\n",
    "            yield StructField(prop, StringType(), 'null' in meta['type'])\n",
    "        elif 'integer' in meta['type']:\n",
    "            yield StructField(prop, IntegerType(), 'null' in meta['type'])\n",
    "        elif 'boolean' in meta['type']:\n",
    "            yield StructField(prop, BooleanType(), 'null' in meta['type'])\n",
    "        elif meta['type'] == 'array' and 'items' not in meta:\n",
    "            # Assuming strings in the array\n",
    "            yield StructField(prop, ArrayType(StringType(), False), True)\n",
    "        elif meta['type'] == 'array' and 'items' in meta:\n",
    "            struct = StructType()\n",
    "            for row in get_rows(meta['items']):\n",
    "                struct.add(row)\n",
    "            yield StructField(prop, ArrayType(struct), True)\n",
    "        elif meta['type'] == 'object':\n",
    "            struct = StructType()\n",
    "            for row in get_rows(meta):\n",
    "                struct.add(row)\n",
    "            yield StructField(prop, struct, True)\n",
    "        else:\n",
    "            log.error(\"Invalid JSON schema: {}\".format(str(meta)[:100]))\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First fetch from the primary source in s3 as per [bug 1312006](https://bugzilla.mozilla.org/show_bug.cgi?id=1312006). We fall back to the github location if this is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "\n",
    "primary_url = \"todo//:primary\"\n",
    "fallback_url = \"https://raw.githubusercontent.com/mozilla/socorro/master/socorro/schemas/crash_report.json\"\n",
    "\n",
    "try:\n",
    "    log.info(\"Fetching latest crash data schema from {}\"\n",
    "             .format(primary_url))\n",
    "    resp = urllib2.urlopen(primary_url)\n",
    "except Exception as e:\n",
    "    log.warning((\"Could not fetch schema from {}: {}\\n\"\n",
    "                 \"Fetching crash data schema from {}\")\n",
    "                .format(primary_url, e, fallback_url))\n",
    "    resp = urllib2.urlopen(fallback_url)\n",
    "\n",
    "data = json.load(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read crash data as json, convert it to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crash_schema = create_struct(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield (end_date - timedelta(n)).strftime(\"%Y%m%d\")\n",
    "\n",
    "def import_day(d):\n",
    "    source_s3path = \"s3://org-allizom-telemetry-crashes/v1/crash_report\"\n",
    "    dest_s3path = \"s3://net-mozaws-prod-us-west-2-pipeline-analysis/mreid/socorro_crash/v1\"\n",
    "    num_partitions = 5\n",
    "    log.info(\"Processing {}, started at {}\".format(d, dt.utcnow()))\n",
    "    cur_source_s3path = \"{}/{}\".format(source_s3path, d)\n",
    "    cur_dest_s3path = \"{}/crash_date={}\".format(dest_s3path, d)\n",
    "    df = sqlContext.read.json(cur_source_s3path, schema=crash_schema)\n",
    "    df.repartition(num_partitions).write.parquet(cur_dest_s3path, mode=\"overwrite\")\n",
    "\n",
    "def backfill(start_date_yyyymmdd):\n",
    "    start_date = dt.strptime(start_date_yyyymmdd, \"%Y%m%d\")\n",
    "    end_date = dt.utcnow() - timedelta(1) # yesterday\n",
    "    for d in daterange(start_date, end_date):\n",
    "        try:\n",
    "            import_day(d)\n",
    "        except Exception as e:\n",
    "            log.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yesterday = dt.strftime(dt.utcnow() - timedelta(1), \"%Y%m%d\")\n",
    "import_day(yesterday)\n",
    "\n",
    "#backfill(\"20160902\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

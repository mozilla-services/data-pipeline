{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import ujson as json\n",
    "import datetime as dt\n",
    "import gzip\n",
    "import os.path\n",
    "import boto # S3\n",
    "import calendar\n",
    "\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick the report we want to generate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "operating_mode = \"weekly\" # either \"weekly\" or \"monthly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_week_range():\n",
    "    today = dt.date.today()\n",
    "    # Get the first day of the past complete week. We need the weeks starting from \"Sunday\", not from \"Monday\",\n",
    "    # so account for that using |(today.weekday() + 1) % 7)|\n",
    "    start_of_week = today - datetime.timedelta(days=((today.weekday() + 1) % 7), weeks=1)\n",
    "    end_of_week = start_of_week + datetime.timedelta(days=6)\n",
    "    return (start_of_week, end_of_week)\n",
    "\n",
    "def get_last_month_range():\n",
    "    today = dt.date.today()\n",
    "    # Get the last day for the previous month.\n",
    "    end_of_last_month = today.replace(day=1) - datetime.timedelta(days=1)\n",
    "    start_of_last_month = end_of_last_month.replace(day=1)\n",
    "    return (start_of_last_month, end_of_last_month)\n",
    "\n",
    "def snap_to_past_sunday(date):\n",
    "    \"\"\" Get the closest, previous Sunday since date. \"\"\"\n",
    "    return date - datetime.timedelta(days=((date.weekday() + 1) % 7))\n",
    "\n",
    "def snap_to_beginning_of_month(date):\n",
    "    \"\"\" Get the date for the first day of this month. \"\"\"\n",
    "    return date.replace(day=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the core pings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, pick a submission range. Either last week or last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_deduped_pings(sub_range):\n",
    "    core_pings = get_pings(sc,\n",
    "                       app=\"Fennec\",\n",
    "                       doc_type=\"core\",\n",
    "                       source_version=\"1\",\n",
    "                       submission_date=sub_range,\n",
    "                       fraction=1.0)\n",
    "\n",
    "    # We don't need the whole ping. Just get the props we want.\n",
    "    subset = get_pings_properties(core_pings, [\"clientId\",\n",
    "                                               \"osversion\",\n",
    "                                               \"profileDate\",\n",
    "                                               \"meta/submissionDate\",\n",
    "                                               \"meta/geoCountry\",\n",
    "                                               \"meta/appUpdateChannel\",\n",
    "                                               \"meta/Timestamp\",\n",
    "                                               \"meta/documentId\"\n",
    "                                              ])\n",
    "\n",
    "    # We can (sadly) have duplicated pings. Apply deduping.\n",
    "    return subset.map(lambda p: (p[\"meta/documentId\"], p))\\\n",
    "                 .reduceByKey(lambda a, b: a)\\\n",
    "                 .map(lambda t: t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the data\n",
    "\n",
    "Fields documentation:\n",
    "* *os_version* - core pings contain the API level, we need to output the codename+version\n",
    "* *geo* - the country code from the country originating the pings. We're only interested in some countries, other countries are grouped as \"Other\"\n",
    "* *channel* - the product channel\n",
    "* *date* - the first day of the aggregated week/month\n",
    "* *actives* - the number of clients that were active that day. It checked 'org.mozilla.appSessions' before, can we simply count the number of core pings submitted on that day (dedupe by client id!)?\n",
    "* *new_records* - profile creation date == submission date\n",
    " * This could not hold due to broken clocks, temporary loss of network, ...\n",
    "* *d1* - how many clients were active at least once after the profile creation date?\n",
    "* *d7* - how many clients were active at least once in the 7 days following profile creation?\n",
    "* *d30* - how many clients were active at least once in the 30 days following profile creation?\n",
    "* *hours* - session duration in hours. Currently 0.\n",
    "* *google, yahoo, bing, other* - Currently 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_country(original_country):\n",
    "    COUNTRIES_OF_INTEREST = set(['US','CA','BR','MX','FR','ES','IT','PL','TR','RU','DE','IN','ID','CN','JP','GB'])\n",
    "    return original_country if original_country in COUNTRIES_OF_INTEREST else 'Other'\n",
    "\n",
    "def android_api_level_to_version(api_level):\n",
    "    \"\"\"\n",
    "    The core ping stores the API level, but we need to display the OS version/codename.\n",
    "    We can map API Level -> Codename, the related information is available there:\n",
    "    https://source.android.com/source/build-numbers.html\n",
    "    \"\"\"\n",
    "    API_MAP = {\n",
    "        '8': 'Froyo (2.2 - 2.2.3)',\n",
    "        '9': 'Gingerbread (2.3 - 2.3.7)',\n",
    "        '10': 'Gingerbread (2.3 - 2.3.7)',\n",
    "        '11': 'Honeycomb (3.0 - 3.2.6)',\n",
    "        '12': 'Honeycomb (3.0 - 3.2.6)',\n",
    "        '13': 'Honeycomb (3.0 - 3.2.6)',\n",
    "        '14': 'Ice Cream Sandwich (4.0 - 4.0.4)',\n",
    "        '15': 'Ice Cream Sandwich (4.0 - 4.0.4)',\n",
    "        '16': 'Jelly Bean (4.1 - 4.3.x)',\n",
    "        '17': 'Jelly Bean (4.1 - 4.3.x)',\n",
    "        '18': 'Jelly Bean (4.1 - 4.3.x)',\n",
    "        '19': 'KitKat (4.4 - 4.4.4)',\n",
    "        '21': 'Lollipop (5.0 - 5.1)',\n",
    "        '22': 'Lollipop (5.0 - 5.1)',\n",
    "        '23': 'Marshmallow (6.0)',\n",
    "    }\n",
    "    return API_MAP[api_level] if api_level in API_MAP else 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_to_unix_days(s):\n",
    "    \"\"\" Converts YYYYMMDD to days since unix epoch \"\"\"\n",
    "    return (dt.datetime.strptime(s, \"%Y%m%d\") - dt.datetime(1970,1,1)).days\n",
    "\n",
    "def get_file_name(report_type, suffix=\"\"):\n",
    "    return \"fennec-v4-\" + operating_mode + suffix + \".csv\"\n",
    "\n",
    "def safe_get(p, key, default_value=0):\n",
    "    \"\"\" Checks if key is in p. If it is, return its value. Otherwise, return default_value. \"\"\"\n",
    "    return p[key] if key in p else default_value\n",
    "\n",
    "def safe_increment(p, key):\n",
    "    \"\"\" Safely increments p[key]. \"\"\"\n",
    "    p[key] = safe_get(p, key, 0) + 1\n",
    "\n",
    "def is_new_profile(submission_epoch, profile_epoch):\n",
    "    \"\"\"\n",
    "    Determines if this is a new profile by checking if the submission date\n",
    "    equals the profile creation date.\n",
    "    \"\"\"\n",
    "    return submission_epoch == profile_epoch\n",
    "\n",
    "def get_key(p, segments, report_type):\n",
    "    \"\"\" Build a key-tuple with the dimensions we want to aggregate. \"\"\"\n",
    "    dims = []\n",
    "    \n",
    "    # Translate the API version to a name.\n",
    "    if 'osversion' in segments:\n",
    "        dims.append(android_api_level_to_version(p.get('osversion')))\n",
    "    else:\n",
    "        dims.append('all')\n",
    "\n",
    "    # Only get some of the countries.\n",
    "    if 'meta/geoCountry' in segments:\n",
    "        dims.append(get_country(p.get('meta/geoCountry')))\n",
    "    else:\n",
    "        dims.append('all')\n",
    "\n",
    "    # Only get some of the countries.\n",
    "    if 'meta/appUpdateChannel' in segments:\n",
    "        dims.append(p.get('meta/appUpdateChannel'))\n",
    "    else:\n",
    "        dims.append('all')\n",
    "        \n",
    "    # Append the date, at last. In the weekly mode, that's the first day of the\n",
    "    # submission week. In the monthly mode, that's the first day of the month.\n",
    "    submission_date = dt.datetime.strptime(p.get(\"meta/submissionDate\"), \"%Y%m%d\")\n",
    "    date_string = \"\"\n",
    "    if report_type == \"monthly\":\n",
    "        date_string = submission_date.strftime(\"%Y%m01\")\n",
    "    else:\n",
    "        first_day = submission_date - datetime.timedelta(days=((submission_date.weekday() + 1) % 7))\n",
    "        date_string = first_day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    dims.append(date_string)\n",
    "        \n",
    "    return tuple(dims)\n",
    "\n",
    "def run_query(pings, segments, report_type):\n",
    "    \"\"\"\n",
    "    Aggregate the pings over the dimensions in \"segments\". We start by generating a key for each\n",
    "    ping by chaining the values of the dimensions of interest. If we don't care about a particular\n",
    "    dimension, its value is set to \"all\".\n",
    "    All the pings belonging to a key are aggregated together.\n",
    "    \"\"\"\n",
    "\n",
    "    # Segment the data by indexing them by dimensions.\n",
    "    segmented_pings = pings.map(lambda p: (get_key(p, segments, report_type), p))\n",
    "    \n",
    "    # We require the profile age to measure the retention. Filter out those pings that don't have it.\n",
    "    filtered = segmented_pings.filter(lambda p: p[1].get(\"profileDate\", None) != None)\n",
    "    \n",
    "    # For metrics like d1, d7, d30, and new_records we need only one core ping per client, per day.\n",
    "    # Generate a new RDD containing only one ping per client, for each day, within the segment:\n",
    "    # Step 1 - Append the client id and submission date to the index key\n",
    "    # Step 2 - ReduceByKey so that we get only one ping per day per client\n",
    "    # Step 3 - Strip off the client id/submission date from the index key\n",
    "    one_per_day = filtered.map(lambda p: ((p[0], p[1].get(\"clientId\"), p[1].get(\"meta/submissionDate\")), p[1]))\\\n",
    "                          .reduceByKey(lambda a, b: a)\\\n",
    "                          .map(lambda p: (p[0][0], p[1]))\n",
    "    \n",
    "    # Compute the aggregated counts.\n",
    "    def retention_seq(acc, v):\n",
    "        if not acc:\n",
    "            acc = {}\n",
    "        \n",
    "        # Please note that retention *WILL* be broken by broken clocks.\n",
    "        submission_epoch = parse_to_unix_days(v['meta/submissionDate'])\n",
    "    \n",
    "        # Check if this ping is on a new profile. If so, increments \"new_records\".\n",
    "        if is_new_profile(submission_epoch, v['profileDate']):\n",
    "            safe_increment(acc, 'new_records')\n",
    "            \n",
    "        # Evaluate the d1, d7, d30 retention metrics. First, get the delta between\n",
    "        # the submission date and the profile creation date.\n",
    "        days_after_creation = submission_epoch -  v['profileDate']\n",
    "\n",
    "        # Is the user still engaged after 1 day (d1)?\n",
    "        if days_after_creation == 1:\n",
    "            safe_increment(acc, 'd1')\n",
    "        \n",
    "        # And after 7 days (d7)?\n",
    "        if days_after_creation <= 7:\n",
    "            safe_increment(acc, 'd7')\n",
    "\n",
    "        # And after 30 days (d30)?\n",
    "        if days_after_creation <= 30:\n",
    "            safe_increment(acc, 'd30')\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def cmb(v1, v2):\n",
    "        # Combine the counts from the two partial dictionaries. Hacky?\n",
    "        return  { k: v1.get(k, 0) + v2.get(k, 0) for k in set(v1) | set(v2) }\n",
    "\n",
    "    retention_defaults = {\n",
    "        'new_records': 0,\n",
    "        'actives': 0,\n",
    "        'd1': 0,\n",
    "        'd7': 0,\n",
    "        'd30': 0,\n",
    "    }\n",
    "    aggregated_retention = one_per_day.aggregateByKey(retention_defaults, retention_seq, cmb)\n",
    "\n",
    "    # For each segment, count how many active clients:\n",
    "    def count_actives(acc, v):\n",
    "        acc[\"actives\"] = acc[\"actives\"] + 1\n",
    "        return acc\n",
    "\n",
    "    # We aggregate the active user count in an object to ease joining\n",
    "    actives_per_segment = segmented_pings.map(lambda r: ((r[0], r[1].get(\"clientId\")), 1))\\\n",
    "                                         .reduceByKey(lambda x,y: x)\\\n",
    "                                         .map(lambda r: (r[0][0], 1))\\\n",
    "                                         .aggregateByKey({\"actives\":0}, count_actives, cmb)\n",
    "    \n",
    "    # Join the RDDs.\n",
    "    merged = aggregated_retention.join(actives_per_segment).mapValues(lambda r: cmb(r[0], r[1]))\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a single CSV file, we execute a series of queries and then serialize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_queries(start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    This function has 3 operating modes:\n",
    "     1. \"weekly\", which aggregates the data from the last full week and appends the results to the weekly CSV;\n",
    "     2. \"monthly\", aggregates the last full month and appends the results to the monthly CSV;\n",
    "     3. \"backfill\", given a start and end date, performs weekly or monthly aggregation over that period and\n",
    "        appends to the CSV file.\n",
    "    \"\"\"\n",
    "    # Each entry represents a different query over a set of dimensions of interest.\n",
    "    QUERIES = [\n",
    "        ['osversion', 'meta/geoCountry', 'meta/appUpdateChannel'],\n",
    "        ['osversion', 'meta/appUpdateChannel'],\n",
    "        ['osversion', 'meta/geoCountry'],\n",
    "        ['meta/geoCountry', 'meta/appUpdateChannel'],\n",
    "        ['osversion'],\n",
    "        ['meta/geoCountry'],\n",
    "        ['meta/appUpdateChannel'],\n",
    "        []\n",
    "    ]\n",
    "\n",
    "    # The cumulative RDD holding all the results.\n",
    "    results = sc.emptyRDD()\n",
    "\n",
    "    # Check start_date and end_date for validity. If invalid, set them for last week/month\n",
    "    date_range = get_last_month_range() if operating_mode is \"monthly\" else get_last_week_range()\n",
    "    if start_date != None and end_date != None:\n",
    "        sd = snap_to_past_sunday(start_date) if operating_mode is \"weekly\"\\\n",
    "                                             else snap_to_beginning_of_month(start_date)\n",
    "        date_range = (sd, end_date)\n",
    "\n",
    "    # Split the submission period in chunks, so we don't run out of resources while aggregating.\n",
    "    chunk_start = date_range[0]\n",
    "    chunk_end = None\n",
    "\n",
    "    while chunk_start <= date_range[1]:\n",
    "        # Compute the end of this time chunk.\n",
    "        if operating_mode == \"monthly\":\n",
    "            chunk_end = chunk_start.replace(day=calendar.monthrange(chunk_start.year, chunk_start.month)[1])\n",
    "        else:\n",
    "            chunk_end = chunk_start + dt.timedelta(days=6)\n",
    "\n",
    "        # Fetch the pings we need.\n",
    "        submissions_range = (chunk_start.strftime(\"%Y%m%d\"), chunk_end.strftime(\"%Y%m%d\"))\n",
    "        deduped = fetch_deduped_pings(submissions_range)\n",
    "\n",
    "        chunk_results = sc.emptyRDD()\n",
    "        \n",
    "        for query in QUERIES:\n",
    "            print \"Running query over dimensions: %s\" % \", \".join(query) \n",
    "            query_result = run_query(deduped, query, operating_mode)\n",
    "            # Append this RDD to the results for this chunk.\n",
    "            chunk_results = chunk_results.union(query_result)\n",
    "\n",
    "\n",
    "        # Serialize intermediate results to file, so we don't start from scratch if the batch fails.\n",
    "        # We append the week/month at the end of the file name.\n",
    "        serialize_results(chunk_results, submissions_range[0])\n",
    "\n",
    "        # Move on to the next chunk, just add one day to either last month or week.\n",
    "        chunk_start = chunk_end + dt.timedelta(days=1)\n",
    "        \n",
    "        # Append this chunk results to the whole RDD. We assume the keys DO NOT collide.\n",
    "        results = results.union(chunk_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV and S3 utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions to read/write from the S3 store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_previous_state():\n",
    "    file_name = get_file_name(operating_mode)\n",
    "\n",
    "    # Fetch the CSV from s3://net-mozaws-prod-metrics-data/fennec-dashboard\n",
    "    conn = boto.connect_s3(host=\"s3-us-west-2.amazonaws.com\")\n",
    "    bucket = conn.get_bucket(\"net-mozaws-prod-metrics-data\")\n",
    "\n",
    "    key = bucket.get_key('fennec-dashboard/' + file_name)\n",
    "    if key is None:\n",
    "        print(\"No previous s3://net-mozaws-prod-metrics-data/fennec-dashboard/%s\" % file_name)\n",
    "        return\n",
    "    \n",
    "    key.get_contents_to_filename(file_name)\n",
    "\n",
    "def store_new_state():\n",
    "    file_name = get_file_name(operating_mode)\n",
    "\n",
    "    # Fetch the CSV from s3://net-mozaws-prod-metrics-data/fennec-dashboard\n",
    "    conn = boto.connect_s3(host=\"s3-us-west-2.amazonaws.com\")\n",
    "    bucket = conn.get_bucket(\"net-mozaws-prod-metrics-data\")\n",
    "\n",
    "    k = Key(bucket)\n",
    "    k.key = 'fennec-dashboard/' + file_name\n",
    "    k.set_contents_from_filename(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map our data to CSV and then save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_csv(r):\n",
    "    # The key itself is a tuple containing the following data:\n",
    "    # (os, geo, channel, date)\n",
    "    data_from_key = r[0]\n",
    "    formatted_date = dt.datetime.strptime(data_from_key[3], \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    return \",\".join([\n",
    "        data_from_key[0], # os\n",
    "        data_from_key[1], # geo\n",
    "        data_from_key[2], # channel\n",
    "        formatted_date,\n",
    "        str(r[1]['actives']),\n",
    "        str(r[1]['new_records']),\n",
    "        str(r[1]['d1']),\n",
    "        str(r[1]['d7']),\n",
    "        str(r[1]['d30']),\n",
    "        \"0\", #str(r[1]['hours']),\n",
    "        \"0\", #str(r[1]['google']),\n",
    "        \"0\", #str(r[1]['yahoo']),\n",
    "        \"0\", #str(r[1]['bing']),\n",
    "        \"0\", #str(r[1]['other']),\n",
    "    ])\n",
    "            \n",
    "def serialize_results(results, file_suffix=\"\"):\n",
    "    file_name = get_file_name(operating_mode, file_suffix)\n",
    "    skip_csv_header = False\n",
    "    \n",
    "    # If the file is already there, append the new data, but don't print the header again.\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"Omitting the CSV header\")\n",
    "        skip_csv_header = True \n",
    "\n",
    "    csv_lines = results.map(to_csv).collect()\n",
    "    print(\"Writing %i new entries\" % len(csv_lines))\n",
    "    with open(file_name, \"a\") as csv_file:\n",
    "        # The file didn't exist before this call, print the header.\n",
    "        if not skip_csv_header:\n",
    "            header = \"os_version,geo,channel,date,actives,new_records,d1,d7,d30,hours,google,yahoo,bing,other\"\n",
    "            csv_file.write(header.encode('utf8') + \"\\n\")\n",
    "        \n",
    "        # Finally append the data lines.\n",
    "        for r in csv_lines:\n",
    "            csv_file.write(r.encode('utf8') + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query over dimensions: osversion, meta/geoCountry, meta/appUpdateChannel\n",
      "Running query over dimensions: osversion, meta/appUpdateChannel\n",
      "Running query over dimensions: osversion, meta/geoCountry\n",
      "Running query over dimensions: meta/geoCountry, meta/appUpdateChannel\n",
      "Running query over dimensions: osversion\n",
      "Running query over dimensions: meta/geoCountry\n",
      "Running query over dimensions: meta/appUpdateChannel\n",
      "Running query over dimensions: "
     ]
    }
   ],
   "source": [
    "if operating_mode not in [\"weekly\", \"monthly\"]:\n",
    "    raise ValueError(\"Unknown operating mode: %s \" % operating_mode)\n",
    "\n",
    "start_date = None # Only use this when backfilling, e.g. dt.datetime(2016,3,6)\n",
    "end_date = None # Only use this when backfilling, e.g. dt.datetime(2016,3,19)\n",
    "# Run the query and compute the results.\n",
    "result = run_queries(start_date, end_date)\n",
    "# Fetch the previous CSV file from S3.\n",
    "fetch_previous_state()\n",
    "# Updates it.\n",
    "serialize_results(result)\n",
    "# Stores the updated one back to S3\n",
    "store_new_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

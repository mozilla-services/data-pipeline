{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting crash-stats for OOM data to S3 (daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "import urllib\n",
    "import math\n",
    "import datetime as dt\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_format = \"%Y-%m-%d\"\n",
    "#target_date = '2016-05-02'\n",
    "results = []\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def safe_str(obj):\n",
    "    \"\"\" Return the byte string representation of obj \"\"\"\n",
    "    if obj is None:\n",
    "        return unicode(\"\")\n",
    "    return unicode(obj)\n",
    "\n",
    "\n",
    "def safe_long(obj):\n",
    "    \"\"\" Return the long representation of obj, or None \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return long(obj)\n",
    "\n",
    "\n",
    "def safe_int(obj):\n",
    "    \"\"\" Return the int representation of obj, or None \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return int(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the crash-stats SuperSearch API automatically paginates, we need to make similar requests in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_API_data(url, params, hdrs):\n",
    "    \"\"\" Returns full API response via JSON \"\"\"\n",
    "    querystring = urllib.urlencode(params, doseq=True)\n",
    "    full_url = url + '?' + querystring\n",
    "    req = urllib2.Request(url=full_url, headers=hdrs)\n",
    "    response = urllib2.urlopen(req)\n",
    "    return json.loads(response.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, this data should be written to S3. I've never done this before (so this is one of the two crucial pieces to test), so please be merciful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_s3(lists, start_date, debug):\n",
    "    s3_output = \"s3n://net-mozaws-prod-us-west-2-pipeline-analysis/\"\n",
    "    s3_output += \"ddurst/crash-stats-oom/submission=\" + start_date\n",
    "    schema = StructType([\n",
    "        StructField(\"uuid\", StringType(), False),\n",
    "        StructField(\"date\", TimestampType(), False),\n",
    "        StructField(\"signature\", StringType(), False),\n",
    "        StructField(\"platform\", StringType(), True),\n",
    "        StructField(\"contains_memory_report\", BooleanType(), True),\n",
    "        StructField(\"oom_allocation_size\", LongType(), True),\n",
    "        StructField(\"system_memory_use_percentage\", IntegerType(), True),\n",
    "        StructField(\"total_virtual_memory\", LongType(), True),\n",
    "        StructField(\"available_virtual_memory\", LongType(), True),\n",
    "        StructField(\"total_page_file\", LongType(), True),\n",
    "        StructField(\"available_page_file\", LongType(), True),\n",
    "        StructField(\"total_physical_memory\", LongType(), True),\n",
    "        StructField(\"available_physical_memory\", LongType(), True),\n",
    "        StructField(\"largest_free_vm_block\", StringType(), True),\n",
    "        StructField(\"largest_free_vm_block_int\", LongType(), True),\n",
    "        StructField(\"tiny_block_size\", LongType(), True),\n",
    "        StructField(\"write_combine_size\", LongType(), True),\n",
    "        StructField(\"shutdown_progress\", StringType(), True),\n",
    "        StructField(\"ipc_channel_error\", StringType(), True),\n",
    "        StructField(\"user_comments\", StringType(), True),\n",
    "    ])\n",
    "    grouped = sqlContext.createDataFrame(lists, schema)\n",
    "    if debug:\n",
    "        grouped.printSchema()\n",
    "    else:\n",
    "        grouped.coalesce(1).write.parquet(s3_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function to setup request data and loop through the required number of pages of results for the date specified (note: it's assumed this would be run once per previous day via cron or the like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_crashstats_by_day(format, debug, specify_date=None):\n",
    "    \"\"\" Get total crashstats data for a specified date.\n",
    "    Will use previous day if not specified. \"\"\"\n",
    "    if specify_date:\n",
    "        start_date = specify_date\n",
    "    else:\n",
    "        start_date = (dt.datetime.now() -\n",
    "                      dt.timedelta(days=1)).strftime(format)\n",
    "    stop_date = (dt.datetime.strptime(start_date, format) +\n",
    "                 dt.timedelta(days=1)).strftime(format)\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Content-Type': 'application/json; charset=UTF-8'\n",
    "    }\n",
    "    target = 'https://crash-stats.mozilla.com/api/SuperSearch/'\n",
    "\n",
    "    param_data = {}\n",
    "    param_data['release_channel'] = \"beta\"\n",
    "    param_data['process_type'] = \"content\"\n",
    "    param_data['_facets'] = \"signature\"\n",
    "    param_data['_results_number'] = \"1\"\n",
    "    param_data['date'] = [\">=\" + start_date, \"<\" + stop_date]\n",
    "    param_data['_columns'] = [\"date\",\n",
    "                              \"uuid\",\n",
    "                              \"signature\",\n",
    "                              \"oom_allocation_size\",\n",
    "                              \"platform\",\n",
    "                              \"contains_memory_report\",\n",
    "                              \"system_memory_use_percentage\",\n",
    "                              \"total_virtual_memory\",\n",
    "                              \"available_virtual_memory\",\n",
    "                              \"total_page_file\",\n",
    "                              \"available_page_file\",\n",
    "                              \"total_physical_memory\",\n",
    "                              \"available_physical_memory\",\n",
    "                              \"largest_free_vm_block\",\n",
    "                              \"tiny_block_size\",\n",
    "                              \"write_combine_size\",\n",
    "                              \"shutdown_progress\",\n",
    "                              \"ipc_channel_error\",\n",
    "                              \"user_comments\"]\n",
    "\n",
    "    # First get the total count\n",
    "    data = get_API_data(target, param_data, headers)\n",
    "    total_results = data[\"total\"]\n",
    "    # Reset limit of results to default, which is 100\n",
    "    per_page_default = 100\n",
    "    param_data['_results_number'] = str(per_page_default)\n",
    "\n",
    "    # Determine the number of pages\n",
    "    pages = (int(math.ceil(total_results/100.0)) * 100) / 100\n",
    "\n",
    "    reqs = 0\n",
    "    offset = 0\n",
    "    all_results = []\n",
    "\n",
    "    # Access each page by offset and append results to the all_results list\n",
    "    while (reqs < pages):\n",
    "        offset = reqs * per_page_default\n",
    "        param_data['_results_offset'] = str(offset)\n",
    "        data = get_API_data(target, param_data, headers)\n",
    "        # Grab the 'hits' into lists\n",
    "        for obj in data[\"hits\"]:\n",
    "            tmp = []\n",
    "            tmp.append(safe_str(obj['uuid']))\n",
    "            # Convert date\n",
    "            tmp.append(dt.datetime.strptime(obj['date'], \"%Y-%m-%dT%H:%M:%S.%f+00:00\"))\n",
    "            tmp.append(safe_str(obj['signature']))\n",
    "            tmp.append(safe_str(obj['platform']))\n",
    "            tmp.append(bool(obj['contains_memory_report']))\n",
    "            tmp.append(safe_long(obj['oom_allocation_size']))\n",
    "            tmp.append(safe_int(obj['system_memory_use_percentage']))\n",
    "            tmp.append(safe_long(obj['total_virtual_memory']))\n",
    "            tmp.append(safe_long(obj['available_virtual_memory']))\n",
    "            tmp.append(safe_long(obj['total_page_file']))\n",
    "            tmp.append(safe_long(obj['available_page_file']))\n",
    "            tmp.append(safe_long(obj['total_physical_memory']))\n",
    "            tmp.append(safe_long(obj['available_physical_memory']))\n",
    "            tmp.append(safe_str(obj['largest_free_vm_block']))\n",
    "            # Add field for non-hex largest_free_vm_block value\n",
    "            if obj['largest_free_vm_block'] is not None:\n",
    "                tmp.append(int(obj['largest_free_vm_block'], 0))\n",
    "            else:\n",
    "                tmp.append(None)\n",
    "            tmp.append(safe_long(obj['tiny_block_size']))\n",
    "            tmp.append(safe_long(obj['write_combine_size']))\n",
    "            tmp.append(safe_str(obj['shutdown_progress']))\n",
    "            tmp.append(safe_str(obj['ipc_channel_error']))\n",
    "            # Handle possible newlines in user_comments\n",
    "            if obj['user_comments'] is not None:\n",
    "                tmp.append(obj['user_comments'].replace(\"\\r\\n\", \"|\").replace(\"\\r\", \"|\").replace(\"\\n\", \"|\"))\n",
    "            else:\n",
    "                tmp.append(None)\n",
    "            all_results.append(tmp)\n",
    "        reqs += 1\n",
    "\n",
    "    write_to_s3(all_results, start_date, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uuid: string (nullable = false)\n",
      " |-- date: timestamp (nullable = false)\n",
      " |-- signature: string (nullable = false)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- contains_memory_report: boolean (nullable = true)\n",
      " |-- oom_allocation_size: long (nullable = true)\n",
      " |-- system_memory_use_percentage: integer (nullable = true)\n",
      " |-- total_virtual_memory: long (nullable = true)\n",
      " |-- available_virtual_memory: long (nullable = true)\n",
      " |-- total_page_file: long (nullable = true)\n",
      " |-- available_page_file: long (nullable = true)\n",
      " |-- total_physical_memory: long (nullable = true)\n",
      " |-- available_physical_memory: long (nullable = true)\n",
      " |-- largest_free_vm_block: string (nullable = true)\n",
      " |-- largest_free_vm_block_int: long (nullable = true)\n",
      " |-- tiny_block_size: long (nullable = true)\n",
      " |-- write_combine_size: long (nullable = true)\n",
      " |-- shutdown_progress: string (nullable = true)\n",
      " |-- ipc_channel_error: string (nullable = true)\n",
      " |-- user_comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_crashstats_by_day(date_format, DEBUG, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

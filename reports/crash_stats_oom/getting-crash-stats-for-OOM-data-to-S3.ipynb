{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting crash-stats for OOM data to S3 (daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "import urllib\n",
    "import math\n",
    "import datetime as dt\n",
    "from pyspark.sql.types import *\n",
    "import boto3.s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_format = \"%Y-%m-%d\"\n",
    "target_date = None\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def safe_str(obj):\n",
    "    \"\"\" Return the byte string representation of obj \"\"\"\n",
    "    if obj is None:\n",
    "        return unicode(\"\")\n",
    "    return unicode(obj)\n",
    "\n",
    "\n",
    "def safe_long(obj):\n",
    "    \"\"\" Return the long representation of obj, or None \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return long(obj)\n",
    "\n",
    "\n",
    "def safe_int(obj):\n",
    "    \"\"\" Return the int representation of obj, or None \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return int(obj)\n",
    "\n",
    "\n",
    "def get_token_from_s3():\n",
    "    \"\"\" Get socorro token from data already saved to s3 \"\"\"\n",
    "    data_bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "    s3path = \"ddurst/oom\"\n",
    "    filename = \"oom.tkn\"\n",
    "    key = \"{}/{}\".format(s3path, filename)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(data_bucket, key)\n",
    "\n",
    "    try:\n",
    "        token = obj.get()['Body'].read()\n",
    "    except:\n",
    "        token = None\n",
    "    \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the crash-stats SuperSearch API automatically paginates, we will need to make similar requests in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_API_data(url, params, hdrs):\n",
    "    \"\"\" Returns full API response via JSON \"\"\"\n",
    "    querystring = urllib.urlencode(params, doseq=True)\n",
    "    full_url = url + '?' + querystring\n",
    "    req = urllib2.Request(url=full_url, headers=hdrs)\n",
    "    response = urllib2.urlopen(req)\n",
    "    return json.loads(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_s3(lists, start_date, debug):\n",
    "    s3_output = \"s3://net-mozaws-prod-us-west-2-pipeline-analysis/\"\n",
    "    s3_output += \"ddurst/crash_stats_oom/v1/submission=\" + start_date\n",
    "    schema = StructType([\n",
    "        StructField(\"uuid\", StringType(), False),\n",
    "        StructField(\"date\", TimestampType(), False),\n",
    "        StructField(\"signature\", StringType(), False),\n",
    "        StructField(\"platform\", StringType(), True),\n",
    "        StructField(\"contains_memory_report\", BooleanType(), True),\n",
    "        StructField(\"oom_allocation_size\", LongType(), True),\n",
    "        StructField(\"system_memory_use_percentage\", IntegerType(), True),\n",
    "        StructField(\"total_virtual_memory\", LongType(), True),\n",
    "        StructField(\"available_virtual_memory\", LongType(), True),\n",
    "        StructField(\"total_page_file\", LongType(), True),\n",
    "        StructField(\"available_page_file\", LongType(), True),\n",
    "        StructField(\"total_physical_memory\", LongType(), True),\n",
    "        StructField(\"available_physical_memory\", LongType(), True),\n",
    "        StructField(\"largest_free_vm_block\", StringType(), True),\n",
    "        StructField(\"largest_free_vm_block_int\", LongType(), True),\n",
    "        StructField(\"tiny_block_size\", LongType(), True),\n",
    "        StructField(\"write_combine_size\", LongType(), True),\n",
    "        StructField(\"shutdown_progress\", StringType(), True),\n",
    "        StructField(\"ipc_channel_error\", StringType(), True),\n",
    "        StructField(\"user_comments\", StringType(), True),\n",
    "    ])\n",
    "    grouped = sqlContext.createDataFrame(lists, schema)\n",
    "    if debug:\n",
    "        grouped.printSchema()\n",
    "    else:\n",
    "        grouped.coalesce(1).write.parquet(s3_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function to setup request data and loop through the required number of pages of results for the date specified (note: it's assumed this would be run once per previous day via cron or the like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_crashstats_by_day(format, debug, specify_date=None):\n",
    "    \"\"\" Get total crashstats data for a specified date.\n",
    "    Will use previous day if not specified. \"\"\"\n",
    "    if specify_date:\n",
    "        start_date = specify_date\n",
    "    else:\n",
    "        start_date = (dt.datetime.now() -\n",
    "                      dt.timedelta(days=1)).strftime(format)\n",
    "    stop_date = (dt.datetime.strptime(start_date, format) +\n",
    "                 dt.timedelta(days=1)).strftime(format)\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Auth-Token': 'c61d715efd1741c3a04711d167b16304',\n",
    "        'Content-Type': 'application/json; charset=UTF-8'\n",
    "    }\n",
    "    # Try to get the token, but if it fails try without it\n",
    "    token = get_token_from_s3()\n",
    "    if token:\n",
    "        headers['Auth-Token'] = token\n",
    "    \n",
    "    target = 'https://crash-stats.mozilla.com/api/SuperSearch/'\n",
    "    per_page_default = 500\n",
    "\n",
    "    param_data = {}\n",
    "    param_data['release_channel'] = \"beta\"\n",
    "    param_data['process_type'] = \"content\"\n",
    "    param_data['_results_number'] = str(per_page_default)\n",
    "    param_data['date'] = [\">=\" + start_date, \"<\" + stop_date]\n",
    "    param_data['_columns'] = [\"date\",\n",
    "                              \"uuid\",\n",
    "                              \"signature\",\n",
    "                              \"oom_allocation_size\",\n",
    "                              \"platform\",\n",
    "                              \"contains_memory_report\",\n",
    "                              \"system_memory_use_percentage\",\n",
    "                              \"total_virtual_memory\",\n",
    "                              \"available_virtual_memory\",\n",
    "                              \"total_page_file\",\n",
    "                              \"available_page_file\",\n",
    "                              \"total_physical_memory\",\n",
    "                              \"available_physical_memory\",\n",
    "                              \"largest_free_vm_block\",\n",
    "                              \"tiny_block_size\",\n",
    "                              \"write_combine_size\",\n",
    "                              \"shutdown_progress\",\n",
    "                              \"ipc_channel_error\",\n",
    "                              \"user_comments\"]\n",
    "\n",
    "    reqs = 0\n",
    "    pages = 1\n",
    "    offset = 0\n",
    "    all_results = []\n",
    "\n",
    "    # Access each page by offset and append results to the all_results list\n",
    "    while (reqs < pages):\n",
    "        offset = reqs * per_page_default\n",
    "        param_data['_results_offset'] = str(offset)\n",
    "        data = get_API_data(target, param_data, headers)\n",
    "        # Determine the number of pages (only on first page request)\n",
    "        if (reqs == 0):\n",
    "            total_results = data[\"total\"]\n",
    "            total_pages = (int(math.ceil(total_results/float(per_page_default))) * per_page_default) / per_page_default\n",
    "            if (total_pages > pages):\n",
    "                pages = total_pages\n",
    "        # Grab the 'hits' into lists\n",
    "        for obj in data[\"hits\"]:\n",
    "            tmp = []\n",
    "            tmp.append(safe_str(obj['uuid']))\n",
    "            # Convert date\n",
    "            tmp.append(dt.datetime.strptime(obj['date'], \"%Y-%m-%dT%H:%M:%S.%f+00:00\"))\n",
    "            tmp.append(safe_str(obj['signature']))\n",
    "            tmp.append(safe_str(obj['platform']))\n",
    "            tmp.append(bool(obj['contains_memory_report']))\n",
    "            tmp.append(safe_long(obj['oom_allocation_size']))\n",
    "            tmp.append(safe_int(obj['system_memory_use_percentage']))\n",
    "            tmp.append(safe_long(obj['total_virtual_memory']))\n",
    "            tmp.append(safe_long(obj['available_virtual_memory']))\n",
    "            tmp.append(safe_long(obj['total_page_file']))\n",
    "            tmp.append(safe_long(obj['available_page_file']))\n",
    "            tmp.append(safe_long(obj['total_physical_memory']))\n",
    "            tmp.append(safe_long(obj['available_physical_memory']))\n",
    "            tmp.append(safe_str(obj['largest_free_vm_block']))\n",
    "            # Add field for non-hex largest_free_vm_block value\n",
    "            if obj['largest_free_vm_block'] is not None:\n",
    "                tmp.append(int(obj['largest_free_vm_block'], 0))\n",
    "            else:\n",
    "                tmp.append(None)\n",
    "            tmp.append(safe_long(obj['tiny_block_size']))\n",
    "            tmp.append(safe_long(obj['write_combine_size']))\n",
    "            tmp.append(safe_str(obj['shutdown_progress']))\n",
    "            tmp.append(safe_str(obj['ipc_channel_error']))\n",
    "            # Handle possible newlines in user_comments\n",
    "            if obj['user_comments'] is not None:\n",
    "                tmp.append(obj['user_comments'].replace(\"\\r\\n\", \"|\").replace(\"\\r\", \"|\").replace(\"\\n\", \"|\"))\n",
    "            else:\n",
    "                tmp.append(None)\n",
    "            all_results.append(tmp)\n",
    "        reqs += 1\n",
    "\n",
    "    write_to_s3(all_results, start_date, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes, when re-running this manually, this is handy.\n",
    "def count_crashstats_by_day(format, debug, specify_date=None):\n",
    "    \"\"\" Get total crashstats data for a specified date.\n",
    "    Will use previous day if not specified. \"\"\"\n",
    "    if specify_date:\n",
    "        start_date = specify_date\n",
    "    else:\n",
    "        start_date = (dt.datetime.now() -\n",
    "                      dt.timedelta(days=1)).strftime(format)\n",
    "    stop_date = (dt.datetime.strptime(start_date, format) +\n",
    "                 dt.timedelta(days=1)).strftime(format)\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Content-Type': 'application/json; charset=UTF-8'\n",
    "    }\n",
    "    # Try to get the token, but if it fails try without it\n",
    "    token = get_token_from_s3()\n",
    "    if token:\n",
    "        headers['Auth-Token'] = token\n",
    "\n",
    "    target = 'https://crash-stats.mozilla.com/api/SuperSearch/'\n",
    "    per_page_default = 500\n",
    "\n",
    "    param_data = {}\n",
    "    param_data['release_channel'] = \"beta\"\n",
    "    param_data['process_type'] = \"content\"\n",
    "    param_data['_results_number'] = str(per_page_default)\n",
    "    param_data['date'] = [\">=\" + start_date, \"<\" + stop_date]\n",
    "    param_data['_columns'] = [\"date\",\n",
    "                              \"uuid\",\n",
    "                              \"signature\",\n",
    "                              \"oom_allocation_size\",\n",
    "                              \"platform\",\n",
    "                              \"contains_memory_report\",\n",
    "                              \"system_memory_use_percentage\",\n",
    "                              \"total_virtual_memory\",\n",
    "                              \"available_virtual_memory\",\n",
    "                              \"total_page_file\",\n",
    "                              \"available_page_file\",\n",
    "                              \"total_physical_memory\",\n",
    "                              \"available_physical_memory\",\n",
    "                              \"largest_free_vm_block\",\n",
    "                              \"tiny_block_size\",\n",
    "                              \"write_combine_size\",\n",
    "                              \"shutdown_progress\",\n",
    "                              \"ipc_channel_error\",\n",
    "                              \"user_comments\"]\n",
    "\n",
    "    reqs = 0\n",
    "    pages = 1\n",
    "    offset = 0\n",
    "    all_results = []\n",
    "\n",
    "    # Access each page by offset and append results to the all_results list\n",
    "    offset = reqs * per_page_default\n",
    "    param_data['_results_offset'] = str(offset)\n",
    "    data = get_API_data(target, param_data, headers)\n",
    "    total_results = data[\"total\"]\n",
    "    total_pages = (int(math.ceil(total_results/float(per_page_default))) * per_page_default) / per_page_default\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_crashstats_by_day(date_format, DEBUG, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Firefox Destop Churn / Retention Cohort analysis\"\n",
    "authors:\n",
    "- mreid-moz\n",
    "- Dexterp37\n",
    "tags:\n",
    "- churn\n",
    "- retention\n",
    "- cohort\n",
    "- firefox desktop\n",
    "- main_summary\n",
    "created_at: 2016-03-28\n",
    "updated_at: 2016-11-30\n",
    "tldr: |\n",
    "    Compute churn / retention information for unique segments of Firefox \n",
    "    users acquired during a specific period of time.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firefox Destop Churn / Retention Cohort analysis\n",
    "\n",
    "Compute churn / retention information for unique segments of Firefox users acquired during a specific period of time. Tracked in [Bug 1226379](https://bugzilla.mozilla.org/show_bug.cgi?id=1226379). The underlying dataset is generated via the [telemetry-batch-view](https://github.com/mozilla/telemetry-batch-view/blob/master/src/main/scala/com/mozilla/telemetry/views/MainSummaryView.scala) code, and is generated once a day.\n",
    "\n",
    "The aggregated churn data is updated weekly.\n",
    "\n",
    "Code is based on the previous [FHR analysis code](https://github.com/mozilla/churn-analysis).\n",
    "\n",
    "Details and definitions are in [Bug 1198537](https://bugzilla.mozilla.org/show_bug.cgi?id=1198537). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many cores are we running on? \n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read source data\n",
    "\n",
    "Read the data from the parquet datastore on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import ujson as json\n",
    "import requests\n",
    "from datetime import datetime as _datetime, timedelta, date\n",
    "import gzip\n",
    "import boto3\n",
    "import botocore\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "from traceback import print_exc\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "bucket = \"telemetry-parquet\"\n",
    "prefix = \"main_summary/v3\"\n",
    "%time dataset = sqlContext.read.load(\"s3://{}/{}\".format(bucket, prefix), \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.select('client_id', 'channel', 'normalized_channel', 'country',\n",
    "                         'profile_creation_date', 'subsession_start_date', \n",
    "                         'subsession_length', 'distribution_id', 'sync_configured', \n",
    "                         'sync_count_desktop', 'sync_count_mobile', 'app_version', \n",
    "                         'timestamp', 'submission_date_s3'\n",
    "                        ).withColumnRenamed('app_version', 'version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- normalized_channel: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- profile_creation_date: integer (nullable = true)\n",
      " |-- subsession_start_date: string (nullable = true)\n",
      " |-- subsession_length: integer (nullable = true)\n",
      " |-- distribution_id: string (nullable = true)\n",
      " |-- sync_configured: boolean (nullable = true)\n",
      " |-- sync_count_desktop: integer (nullable = true)\n",
      " |-- sync_count_mobile: integer (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- submission_date_s3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What do the records look like?\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "Define some helper functions for reorganizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Bug 1289573: Support values like \"mozilla86\" and \"mozilla86-utility-existing\"\n",
    "funnelcake_pattern = re.compile(\"^mozilla[0-9]+.*$\")\n",
    "\n",
    "def get_effective_version(d2v, channel, day):\n",
    "    \"\"\" Get the effective version on the given channel on the given day.\"\"\"\n",
    "    if day not in d2v:\n",
    "        if day < \"2015-01-01\":\n",
    "            return \"older\"\n",
    "        else:\n",
    "            return \"newer\"\n",
    "\n",
    "    effective_version = d2v[day]\n",
    "    return get_channel_version(channel, effective_version)\n",
    "\n",
    "def get_channel_version(channel, version):\n",
    "    \"\"\" Given a channel and an effective release-channel version, give the\n",
    "    calculated channel-specific version.\"\"\"\n",
    "    if channel.startswith('release'):\n",
    "        return version\n",
    "    numeric_version = int(version[0:version.index('.')])\n",
    "    offset = 0\n",
    "    if channel.startswith('beta'):\n",
    "        offset = 1\n",
    "    elif channel.startswith('aurora'):\n",
    "        offset = 2\n",
    "    elif channel.startswith('nightly'):\n",
    "        offset = 3\n",
    "    return \"{}.0\".format(numeric_version + offset)\n",
    "\n",
    "def make_d2v(release_info):\n",
    "    \"\"\" Create a map of yyyy-mm-dd date to the effective Firefox version on the\n",
    "    release channel.\n",
    "    \"\"\"\n",
    "    # Combine major and minor releases into a map of day -> version\n",
    "    # Keep only the highest version available for a day range.\n",
    "    observed_dates = set(release_info[\"major\"].values())\n",
    "    observed_dates |= set(release_info[\"minor\"].values())\n",
    "    # Skip old versions.\n",
    "    sd = [ d for d in sorted(observed_dates) if d >= \"2014-01-01\" ]\n",
    "    start_date_str = sd[0]\n",
    "    start_date = _datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = _datetime.strptime(sd[-1], \"%Y-%m-%d\")\n",
    "    day_count = (end_date - start_date).days + 1\n",
    "    d2v = {}\n",
    "    # Start with all the available version release days:\n",
    "    for t in [\"major\", \"minor\"]:\n",
    "        for m, d in release_info[t].iteritems():\n",
    "            if d < start_date_str:\n",
    "                continue\n",
    "            if d not in d2v or compare_ver(m, d2v[d]) > 0:\n",
    "                d2v[d] = m\n",
    "    last_ver = d2v[start_date_str]\n",
    "    # Fill in all the gaps:\n",
    "    for dt in (start_date + timedelta(n) for n in range(day_count)):\n",
    "        d = _datetime.strftime(dt, \"%Y-%m-%d\")\n",
    "        if d in d2v:\n",
    "            # Don't replace a higher version with a new release of an old\n",
    "            # version (probably an ESR release)\n",
    "            if compare_ver(d2v[d], last_ver) < 0:\n",
    "                d2v[d] = last_ver\n",
    "            else:\n",
    "                last_ver = d2v[d]\n",
    "        else:\n",
    "            d2v[d] = last_ver\n",
    "    return d2v\n",
    "\n",
    "def fetch_json(uri):\n",
    "    \"\"\" Perform an HTTP GET on the given uri, return the results as json. If\n",
    "    there is an error fetching the data, raise it.\n",
    "    \"\"\"\n",
    "    data = requests.get(uri)\n",
    "    # Raise an exception if the fetch failed.\n",
    "    data.raise_for_status()\n",
    "    return data.json()\n",
    "\n",
    "def compare_ver(a, b):\n",
    "    \"\"\" Logically compare two Firefox version strings. Split the string into\n",
    "    pieces, and compare each piece numerically.\n",
    "\n",
    "    Returns -1, 0, or 1 depending on whether a is less than, equal to, or\n",
    "    greater than b.\n",
    "    \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "\n",
    "    ap = [int(p) for p in a.split(\".\")]\n",
    "    bp = [int(p) for p in b.split(\".\")]\n",
    "    lap = len(ap)\n",
    "    lbp = len(bp)\n",
    "\n",
    "    # min # of pieces\n",
    "    mp = lap\n",
    "    if lbp < mp:\n",
    "        mp = lbp\n",
    "\n",
    "    for i in range(mp):\n",
    "        if ap[i] < bp[i]:\n",
    "            return -1\n",
    "        if ap[i] > bp[i]:\n",
    "            return 1\n",
    "\n",
    "    if lap > lbp:\n",
    "        # a has more pieces, common pieces are the same, a is greater\n",
    "        return 1\n",
    "\n",
    "    if lap < lbp:\n",
    "        # a has fewer pieces, common pieces are the same, b is greater\n",
    "        return -1\n",
    "\n",
    "    # They are exactly the same.\n",
    "    return 0\n",
    "\n",
    "def get_release_info():\n",
    "    \"\"\" Fetch information about Firefox release dates. Returns an object\n",
    "    containing two sections:\n",
    "\n",
    "    'major' - contains major releases (i.e. 41.0)\n",
    "    'minor' - contains stability releases (i.e. 41.0.1)\n",
    "    \"\"\"\n",
    "    major_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_major_releases.json\")\n",
    "    if major_info is None:\n",
    "        raise Exception(\"Failed to fetch major version info\")\n",
    "    minor_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_stability_releases.json\")\n",
    "    if minor_info is None:\n",
    "        raise Exception(\"Failed to fetch minor version info\")\n",
    "    return {\"major\": major_info, \"minor\": minor_info}\n",
    "\n",
    "def daynum_to_date(daynum):\n",
    "    \"\"\" Convert a number of days to a date. If it's out of range, default to a max date.\n",
    "    :param daynum: A number of days since Jan 1, 1970\n",
    "    \"\"\"\n",
    "    if daynum is None:\n",
    "        return None\n",
    "    if daynum < 0:\n",
    "        return None\n",
    "    daycount = int(daynum)\n",
    "    if daycount > 1000000:\n",
    "        # Some time in the 48th century, clearly bogus.\n",
    "        daycount = 1000000\n",
    "    return date(1970, 1, 1) + timedelta(daycount)\n",
    "\n",
    "def sane_date(d):\n",
    "    \"\"\" Check if the given date looks like a legitimate time on which activity\n",
    "    could have happened.\n",
    "    \"\"\"\n",
    "    if d is None:\n",
    "        return False\n",
    "    return d > date(2000, 1, 1) and d < _datetime.utcnow().date() + timedelta(2)\n",
    "\n",
    "def is_funnelcake(distro):\n",
    "    \"\"\" Check if a given distribution_id appears to be a funnelcake build.\"\"\"\n",
    "    if distro is None:\n",
    "        return False\n",
    "    return funnelcake_pattern.match(distro) is not None\n",
    "\n",
    "top_countries = set([\"US\", \"DE\", \"FR\", \"RU\", \"BR\", \"IN\", \"PL\", \"ID\", \"GB\", \"CN\",\n",
    "                  \"IT\", \"JP\", \"CA\", \"ES\", \"UA\", \"MX\", \"AU\", \"VN\", \"EG\", \"AR\",\n",
    "                  \"PH\", \"NL\", \"IR\", \"CZ\", \"HU\", \"TR\", \"RO\", \"GR\", \"AT\", \"CH\"])\n",
    "\n",
    "def top_country(country):\n",
    "    global top_countries\n",
    "    if(country in top_countries):\n",
    "        return country\n",
    "    return \"ROW\"\n",
    "\n",
    "def most_recent_sunday(d):\n",
    "    \"\"\" Get the date corresponding to the Sunday on or before the given date.\"\"\"\n",
    "    if d is None:\n",
    "        return None\n",
    "    weekday = d.weekday()\n",
    "    if weekday == 6:\n",
    "        return d\n",
    "    return d - timedelta(weekday + 1)\n",
    "\n",
    "def get_week_num(creation, today):\n",
    "    if creation is None or today is None:\n",
    "        return None\n",
    "\n",
    "    diff = (today.date() - creation).days\n",
    "    if diff < 0:\n",
    "        # Creation date is in the future. Bad data :(\n",
    "        return -1\n",
    "    # The initial week is week zero.\n",
    "    return int(diff / 7)\n",
    "\n",
    "# The number of seconds in a single hour, casted to float, so we get the fractional part\n",
    "# when converting.\n",
    "SECONDS_IN_HOUR = float(60 * 60)\n",
    "\n",
    "def convert(d2v, week_start, datum):\n",
    "    out = {\"good\": False}\n",
    "\n",
    "    pcd = daynum_to_date(datum.profile_creation_date)\n",
    "    if not sane_date(pcd):\n",
    "        return out\n",
    "\n",
    "    pcd_formatted = _datetime.strftime(pcd, \"%Y-%m-%d\")\n",
    "\n",
    "    out[\"client_id\"] = datum.client_id\n",
    "    channel = datum.normalized_channel\n",
    "    out[\"is_funnelcake\"] = is_funnelcake(datum.distribution_id)\n",
    "    if out[\"is_funnelcake\"]:\n",
    "        channel = \"{}-cck-{}\".format(datum.normalized_channel, datum.distribution_id)\n",
    "    out[\"channel\"] = channel\n",
    "    out[\"geo\"] = top_country(datum.country)\n",
    "    out[\"acquisition_period\"] = most_recent_sunday(pcd)\n",
    "    out[\"start_version\"] = get_effective_version(d2v, channel, pcd_formatted)\n",
    "\n",
    "    deviceCount = 0\n",
    "    if datum.sync_count_desktop is not None:\n",
    "        deviceCount += datum.sync_count_desktop\n",
    "    if datum.sync_count_mobile is not None:\n",
    "        deviceCount += datum.sync_count_mobile\n",
    "            \n",
    "    if deviceCount > 1:\n",
    "        out[\"sync_usage\"] = \"multiple\"\n",
    "    elif deviceCount == 1:\n",
    "        out[\"sync_usage\"] = \"single\"\n",
    "    elif datum.sync_configured is not None:\n",
    "        if datum.sync_configured:\n",
    "            out[\"sync_usage\"] = \"single\"\n",
    "        else:\n",
    "            out[\"sync_usage\"] = \"no\"\n",
    "    # Else we don't set sync_usage at all, and use a default value later.\n",
    "    \n",
    "    out[\"current_version\"] = datum.version\n",
    "    \n",
    "    # The usage time is in seconds, but we really need hours.\n",
    "    # Because we filter out broken subsession_lengths, we could end up with clients with no\n",
    "    # usage hours.\n",
    "    out[\"usage_hours\"] = (datum.usage_seconds / SECONDS_IN_HOUR) if datum.usage_seconds is not None else 0.0\n",
    "    out[\"squared_usage_hours\"] = out[\"usage_hours\"] ** 2\n",
    "    \n",
    "    # Incoming subsession_start_date looks like \"2016-02-22T00:00:00.0-04:00\"\n",
    "    client_date = None\n",
    "    if datum.subsession_start_date is not None:\n",
    "        try:\n",
    "            client_date = _datetime.strptime(datum.subsession_start_date[0:10], \"%Y-%m-%d\")\n",
    "        except ValueError as e1:\n",
    "            # Bogus format\n",
    "            pass\n",
    "        except TypeError as e2:\n",
    "            # String contains null bytes or other weirdness. Example:\n",
    "            # TypeError: must be string without null bytes, not unicode\n",
    "            pass\n",
    "    if client_date is None:\n",
    "        # Fall back to submission date\n",
    "        client_date = _datetime.strptime(datum.submission_date_s3, \"%Y%m%d\")\n",
    "    out[\"current_week\"] = get_week_num(pcd, client_date)\n",
    "    out[\"is_active\"] = \"yes\"\n",
    "    if client_date is not None:\n",
    "        try:\n",
    "            if _datetime.strftime(client_date, \"%Y%m%d\") < week_start:\n",
    "                out[\"is_active\"] = \"no\"\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "    out[\"good\"] = True\n",
    "    return out\n",
    "\n",
    "def csv(f):\n",
    "    return \",\".join([ unicode(a) for a in f[0] ] + [ unicode(a) for a in f[1] ])\n",
    "\n",
    "# Build the \"effective version\" cache:\n",
    "d2v = make_d2v(get_release_info())\n",
    "\n",
    "def get_churn_filename(week_start, week_end):\n",
    "    return \"churn-{}-{}.by_activity.csv.gz\".format(week_start, week_end)\n",
    "\n",
    "def get_churn_filepath():\n",
    "    return \"mreid/churn\"\n",
    "\n",
    "def fmt(d, date_format=\"%Y%m%d\"):\n",
    "    return _datetime.strftime(d, date_format)\n",
    "\n",
    "def exists(s3client, bucket, s, e):\n",
    "    churn_key = \"{}/{}\".format(get_churn_filepath(), get_churn_filename(s, e))\n",
    "    try:\n",
    "        s3client.head_object(Bucket=bucket, Key=churn_key)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            return False\n",
    "        else:\n",
    "            raise e\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the aggregates\n",
    "\n",
    "Run the aggregation code, detecting files that are missing.\n",
    "\n",
    "The fields we want in the output are:\n",
    " - channel (appUpdateChannel)\n",
    " - geo (bucketed into top 30 countries + \"rest of world\")\n",
    " - is_funnelcake (contains \"-cck-\"?)\n",
    " - acquisition_period (cohort_week)\n",
    " - start_version (effective version on profile creation date)\n",
    " - sync_usage (\"no\", \"single\" or \"multiple\" devices)\n",
    " - current_version (current appVersion)\n",
    " - current_week (week)\n",
    " - is_active (were the client_ids active this week or not)\n",
    " - n_profiles (count of matching client_ids)\n",
    " - usage_hours (sum of the per-client subsession lengths, clamped in the [0, MAX_SUBSESSION_LENGTH] range)\n",
    " - sum_squared_usage_hours (the sum of squares of the usage hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week of 20151101 to 20151107 already exists\n",
      "Week of 20151108 to 20151114 already exists\n",
      "Week of 20151115 to 20151121 already exists\n",
      "Week of 20151122 to 20151128 already exists\n",
      "Starting week from 20151129 to 20151205 at 2016-02-25 17:51:10.877731\n",
      "2016-02-25 17:51:11.008565: collecting aggregates\n",
      "2016-02-25 17:56:37.385371: done collecting aggregates\n",
      "2016-02-25 17:56:37.386151: Writing output to churn-20151129-20151205.csv.gz.by_activity5\n",
      "2016-02-25 17:56:37.386378: Wrote header to churn-20151129-20151205.csv.gz.by_activity5\n",
      "2016-02-25 17:56:48.245838: finished writing lines\n",
      "Finished week from 20151129 to 20151205 at 2016-02-25 17:56:48.449721\n",
      "Starting week from 20151206 to 20151212 at 2016-02-25 17:56:48.450176\n",
      "2016-02-25 17:56:48.576869: collecting aggregates\n",
      "2016-02-25 18:02:16.869842: done collecting aggregates\n",
      "2016-02-25 18:02:16.870523: Writing output to churn-20151206-20151212.csv.gz.by_activity5\n",
      "2016-02-25 18:02:16.870742: Wrote header to churn-20151206-20151212.csv.gz.by_activity5\n",
      "2016-02-25 18:02:27.844529: finished writing lines\n",
      "Finished week from 20151206 to 20151212 at 2016-02-25 18:02:28.005866\n",
      "Starting week from 20151213 to 20151219 at 2016-02-25 18:02:28.006440\n",
      "2016-02-25 18:02:28.137427: collecting aggregates\n",
      "2016-02-25 18:07:52.782727: done collecting aggregates\n",
      "2016-02-25 18:07:52.783551: Writing output to churn-20151213-20151219.csv.gz.by_activity5\n",
      "2016-02-25 18:07:52.783779: Wrote header to churn-20151213-20151219.csv.gz.by_activity5\n",
      "2016-02-25 18:08:08.672998: finished writing lines\n",
      "Finished week from 20151213 to 20151219 at 2016-02-25 18:08:08.862909\n",
      "Starting week from 20151220 to 20151226 at 2016-02-25 18:08:08.863503\n",
      "2016-02-25 18:08:08.991054: collecting aggregates\n",
      "2016-02-25 18:13:21.106278: done collecting aggregates\n",
      "2016-02-25 18:13:21.107007: Writing output to churn-20151220-20151226.csv.gz.by_activity5\n",
      "2016-02-25 18:13:21.107237: Wrote header to churn-20151220-20151226.csv.gz.by_activity5\n",
      "2016-02-25 18:13:40.233727: finished writing lines\n",
      "Finished week from 20151220 to 20151226 at 2016-02-25 18:13:40.470278\n",
      "Starting week from 20151227 to 20160102 at 2016-02-25 18:13:40.470792\n",
      "2016-02-25 18:13:40.598394: collecting aggregates\n",
      "2016-02-25 18:18:50.421359: done collecting aggregates\n",
      "2016-02-25 18:18:50.422046: Writing output to churn-20151227-20160102.csv.gz.by_activity5\n",
      "2016-02-25 18:18:50.422273: Wrote header to churn-20151227-20160102.csv.gz.by_activity5\n",
      "2016-02-25 18:19:11.885004: finished writing lines\n",
      "Finished week from 20151227 to 20160102 at 2016-02-25 18:19:12.068829\n",
      "Starting week from 20160103 to 20160109 at 2016-02-25 18:19:12.069415\n",
      "2016-02-25 18:19:12.197376: collecting aggregates\n",
      "2016-02-25 18:24:49.537347: done collecting aggregates\n",
      "2016-02-25 18:24:49.538047: Writing output to churn-20160103-20160109.csv.gz.by_activity5\n",
      "2016-02-25 18:24:49.538270: Wrote header to churn-20160103-20160109.csv.gz.by_activity5\n",
      "2016-02-25 18:25:12.344934: finished writing lines\n",
      "Finished week from 20160103 to 20160109 at 2016-02-25 18:25:12.857178\n",
      "Starting week from 20160110 to 20160116 at 2016-02-25 18:25:12.857770\n",
      "2016-02-25 18:25:13.040382: collecting aggregates\n",
      "2016-02-25 18:31:05.980641: done collecting aggregates\n",
      "2016-02-25 18:31:05.981293: Writing output to churn-20160110-20160116.csv.gz.by_activity5\n",
      "2016-02-25 18:31:05.981511: Wrote header to churn-20160110-20160116.csv.gz.by_activity5\n",
      "2016-02-25 18:31:27.072130: finished writing lines\n",
      "Finished week from 20160110 to 20160116 at 2016-02-25 18:31:27.307505\n",
      "Starting week from 20160117 to 20160123 at 2016-02-25 18:31:27.307976\n",
      "2016-02-25 18:31:27.432604: collecting aggregates\n",
      "2016-02-25 18:37:09.561716: done collecting aggregates\n",
      "2016-02-25 18:37:09.562399: Writing output to churn-20160117-20160123.csv.gz.by_activity5\n",
      "2016-02-25 18:37:09.562628: Wrote header to churn-20160117-20160123.csv.gz.by_activity5\n",
      "2016-02-25 18:37:29.982858: finished writing lines\n",
      "Finished week from 20160117 to 20160123 at 2016-02-25 18:37:30.193391\n",
      "Week of 20160124 to 20160130 already exists\n",
      "Week of 20160131 to 20160206 already exists\n",
      "Week of 20160207 to 20160213 already exists\n",
      "Week of 20160214 to 20160220 already exists\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "def get_newest_per_client(df):\n",
    "    windowSpec = Window.partitionBy(df['client_id']).orderBy(df['timestamp'].desc())\n",
    "    # Note: use 'rowNumber' instead of 'row_number' with Spark < v1.6\n",
    "    rownum_by_timestamp = (func.row_number().over(windowSpec))\n",
    "    selectable_by_client = df.select(\n",
    "        df['client_id'],\n",
    "        df['channel'],\n",
    "        df['normalized_channel'],\n",
    "        df['country'],\n",
    "        df['profile_creation_date'],\n",
    "        df['subsession_start_date'],\n",
    "        df['submission_date_s3'],\n",
    "        df['sync_configured'],\n",
    "        df['sync_count_desktop'],\n",
    "        df['sync_count_mobile'],\n",
    "        df['distribution_id'],\n",
    "        df['version'],\n",
    "        df['timestamp'],\n",
    "        rownum_by_timestamp.alias('row_number')\n",
    "    )\n",
    "    return selectable_by_client.filter(selectable_by_client['row_number'] == 1)\n",
    "\n",
    "DO_UPLOAD = True\n",
    "\n",
    "world_start = '20151101'\n",
    "today = _datetime.utcnow()\n",
    "todays = fmt(today)\n",
    "wsd = _datetime.strptime(world_start, \"%Y%m%d\")\n",
    "week_start_date = wsd\n",
    "week_end_date = week_start_date + timedelta(6)\n",
    "week_start = fmt(week_start_date)\n",
    "week_end = fmt(week_end_date)\n",
    "\n",
    "client = boto3.client('s3', 'us-west-2')\n",
    "transfer = S3Transfer(client)\n",
    "bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "\n",
    "MAX_SUBSESSION_LENGTH = 60 * 60 * 48 # 48 hours in seconds.\n",
    "\n",
    "df = dataset\n",
    "# Stop at the last complete week\n",
    "while week_end < todays:\n",
    "    # Allow this many days for data for a given activity period to arrive.\n",
    "    week_end_slop = fmt(week_end_date + timedelta(10))\n",
    "    \n",
    "    # If the data for this week can still be coming, don't try to compute the churn.\n",
    "    # That also means we can break out of the loop.\n",
    "    if week_end_slop >= todays:\n",
    "        print \"Skipping week of {} to {} - Data is still arriving until {}.\".format(week_start, week_end, week_end_slop)\n",
    "        break\n",
    "    \n",
    "    # Compute missing data periods.\n",
    "    if exists(client, bucket, week_start, week_end):\n",
    "        print \"Week of {} to {} already exists\".format(week_start, week_end)\n",
    "    else:\n",
    "        print \"Starting week from {} to {} at {}\".format(week_start, week_end, _datetime.utcnow())\n",
    "        # the subsession_start_date field has a different form than submission_date_s3,\n",
    "        # so needs to be formatted with hyphens.\n",
    "        week_end_excl = fmt(week_end_date + timedelta(1), date_format=\"%Y-%m-%d\")\n",
    "        week_start_hyphenated = fmt(week_start_date, date_format=\"%Y-%m-%d\")\n",
    "        \n",
    "        current_week = df.filter(df['submission_date_s3'] >= week_start).filter(df['submission_date_s3'] <= week_end_slop).filter(df['subsession_start_date'] >= week_start_hyphenated).filter(df['subsession_start_date'] < week_end_excl)\n",
    "        newest_per_client = get_newest_per_client(current_week)\n",
    "\n",
    "        # Clamp broken subsession values in the [0, MAX_SUBSESSION_LENGTH] range.\n",
    "        clamped_subsession = current_week.select(current_week['client_id'],\n",
    "                                                 func.when(current_week['subsession_length'] > MAX_SUBSESSION_LENGTH, MAX_SUBSESSION_LENGTH)\\\n",
    "                                                     .otherwise(func.when(current_week['subsession_length'] < 0, 0).otherwise(current_week['subsession_length']))\\\n",
    "                                                     .alias('subsession_length'))\n",
    "        \n",
    "        # Compute the overall usage time for each client by summing the subsession lengths.\n",
    "        grouped_usage_time = clamped_subsession.groupby('client_id')\\\n",
    "                                               .sum('subsession_length')\\\n",
    "                                               .withColumnRenamed('sum(subsession_length)', 'usage_seconds')\n",
    "\n",
    "        # Append this column to the original data frame.\n",
    "        newest_with_usage = newest_per_client.join(grouped_usage_time, 'client_id', 'inner')\n",
    "        \n",
    "        converted = newest_with_usage.map(lambda x: convert(d2v, week_start, x))\n",
    "\n",
    "        # Don't bother to filter out non-good records - they will appear \n",
    "        # as 'unknown' in the output.\n",
    "        countable = converted.map(lambda x: ((\n",
    "                    x.get('channel', 'unknown'),\n",
    "                    x.get('geo', 'unknown'),\n",
    "                    \"yes\" if x.get('is_funnelcake', False) else \"no\",\n",
    "                    _datetime.strftime(x.get('acquisition_period', date(2000, 1, 1)), \"%Y-%m-%d\"),\n",
    "                    x.get('start_version', 'unknown'),\n",
    "                    x.get('sync_usage', 'unknown'),\n",
    "                    x.get('current_version', 'unknown'),\n",
    "                    x.get('current_week', 'unknown'),\n",
    "                    x.get('is_active', 'unknown')), (1, x.get('usage_hours', 0), x.get('squared_usage_hours', 0))))\n",
    "\n",
    "        def reduce_func(x, y):\n",
    "            return (x[0] + y[0], # Sum active users\n",
    "                    x[1] + y[1], # Sum usage_hours\n",
    "                    x[2] + y[2]) # Sum squared_usage_hours\n",
    "        aggregated = countable.reduceByKey(reduce_func)\n",
    "        churn_outfile = get_churn_filename(week_start, week_end)\n",
    "        \n",
    "        print \"{}: collecting aggregates\".format(_datetime.utcnow())\n",
    "        records = aggregated.collect()\n",
    "        print \"{}: done collecting aggregates\".format(_datetime.utcnow())\n",
    "        print \"{}: Writing output to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "\n",
    "        # Write the file out as gzipped csv\n",
    "        with gzip.open(churn_outfile, 'wb') as fout:\n",
    "            fout.write(\"channel,geo,is_funnelcake,acquisition_period,start_version,sync_usage,current_version,current_week,is_active,n_profiles,usage_hours,sum_squared_usage_hours\\n\")\n",
    "            print \"{}: Wrote header to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "            for r in records:\n",
    "                try:\n",
    "                    fout.write(csv(r))\n",
    "                    fout.write(\"\\n\")\n",
    "                except UnicodeEncodeError as e:\n",
    "                    print \"{}: Error writing line: {} // {}\".format(_datetime.utcnow(), e, r)\n",
    "            print \"{}: finished writing lines\".format(_datetime.utcnow())\n",
    "\n",
    "        # Now upload it to S3:\n",
    "        if DO_UPLOAD:\n",
    "            churn_s3 = \"{}/{}\".format(get_churn_filepath(), churn_outfile)\n",
    "            transfer.upload_file(churn_outfile, bucket, churn_s3,\n",
    "                                 extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "            \n",
    "            # Also upload it to the dashboard:\n",
    "            # Update the dashboard file\n",
    "            dash_bucket = \"net-mozaws-prod-metrics-data\"\n",
    "            dash_s3_name = \"telemetry-churn/{}\".format(churn_outfile)\n",
    "            transfer.upload_file(churn_outfile, dash_bucket, dash_s3_name,\n",
    "                                 extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "        \n",
    "        print \"Finished week from {} to {} at {}\".format(week_start, week_end, _datetime.utcnow())\n",
    "    # Move forward by a week\n",
    "    week_start_date += timedelta(7)\n",
    "    week_end_date += timedelta(7)\n",
    "    week_start = fmt(week_start_date)\n",
    "    week_end = fmt(week_end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Firefox Destop Churn / Retention Cohort analysis\"\n",
    "authors:\n",
    "- mreid-moz\n",
    "- Dexterp37\n",
    "- acmiyaguchi\n",
    "tags:\n",
    "- churn\n",
    "- retention\n",
    "- cohort\n",
    "- firefox desktop\n",
    "- main_summary\n",
    "created_at: 2016-03-28\n",
    "updated_at: 2016-11-30\n",
    "tldr: |\n",
    "    Compute churn / retention information for unique segments of Firefox \n",
    "    users acquired during a specific period of time.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firefox Destop Churn / Retention Cohort analysis\n",
    "\n",
    "Compute churn / retention information for unique segments of Firefox users acquired during a specific period of time. Tracked in [Bug 1226379](https://bugzilla.mozilla.org/show_bug.cgi?id=1226379). The underlying dataset is generated via the [telemetry-batch-view](https://github.com/mozilla/telemetry-batch-view/blob/master/src/main/scala/com/mozilla/telemetry/views/MainSummaryView.scala) code, and is generated once a day.\n",
    "\n",
    "The aggregated churn data is updated weekly.\n",
    "\n",
    "Code is based on the previous [FHR analysis code](https://github.com/mozilla/churn-analysis).\n",
    "\n",
    "Details and definitions are in [Bug 1198537](https://bugzilla.mozilla.org/show_bug.cgi?id=1198537). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many cores are we running on? \n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read source data\n",
    "\n",
    "Read the data from the parquet datastore on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import ujson as json\n",
    "import requests\n",
    "from datetime import datetime as _datetime, timedelta, date\n",
    "import gzip\n",
    "import boto3\n",
    "import botocore\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "from traceback import print_exc\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "bucket = \"telemetry-parquet\"\n",
    "prefix = \"main_summary/v3\"\n",
    "%time dataset = sqlContext.read.load(\"s3://{}/{}\".format(bucket, prefix), \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = dataset.select('client_id', 'channel', 'normalized_channel', 'country',\n",
    "                         'profile_creation_date', 'subsession_start_date', \n",
    "                         'subsession_length', 'distribution_id', 'sync_configured', \n",
    "                         'sync_count_desktop', 'sync_count_mobile', 'app_version', \n",
    "                         'timestamp', 'submission_date_s3'\n",
    "                        ).withColumnRenamed('app_version', 'version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What do the records look like?\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "Define some helper functions for reorganizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Bug 1289573: Support values like \"mozilla86\" and \"mozilla86-utility-existing\"\n",
    "funnelcake_pattern = re.compile(\"^mozilla[0-9]+.*$\")\n",
    "\n",
    "def get_effective_version(d2v, channel, day):\n",
    "    \"\"\" Get the effective version on the given channel on the given day.\"\"\"\n",
    "    if day not in d2v:\n",
    "        if day < \"2015-01-01\":\n",
    "            return \"older\"\n",
    "        else:\n",
    "            return \"newer\"\n",
    "\n",
    "    effective_version = d2v[day]\n",
    "    return get_channel_version(channel, effective_version)\n",
    "\n",
    "def get_channel_version(channel, version):\n",
    "    \"\"\" Given a channel and an effective release-channel version, give the\n",
    "    calculated channel-specific version.\"\"\"\n",
    "    if channel.startswith('release'):\n",
    "        return version\n",
    "    numeric_version = int(version[0:version.index('.')])\n",
    "    offset = 0\n",
    "    if channel.startswith('beta'):\n",
    "        offset = 1\n",
    "    elif channel.startswith('aurora'):\n",
    "        offset = 2\n",
    "    elif channel.startswith('nightly'):\n",
    "        offset = 3\n",
    "    return \"{}.0\".format(numeric_version + offset)\n",
    "\n",
    "def make_d2v(release_info):\n",
    "    \"\"\" Create a map of yyyy-mm-dd date to the effective Firefox version on the\n",
    "    release channel.\n",
    "    \"\"\"\n",
    "    # Combine major and minor releases into a map of day -> version\n",
    "    # Keep only the highest version available for a day range.\n",
    "    observed_dates = set(release_info[\"major\"].values())\n",
    "    observed_dates |= set(release_info[\"minor\"].values())\n",
    "    # Skip old versions.\n",
    "    sd = [ d for d in sorted(observed_dates) if d >= \"2014-01-01\" ]\n",
    "    start_date_str = sd[0]\n",
    "    start_date = _datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = _datetime.strptime(sd[-1], \"%Y-%m-%d\")\n",
    "    day_count = (end_date - start_date).days + 1\n",
    "    d2v = {}\n",
    "    # Start with all the available version release days:\n",
    "    for t in [\"major\", \"minor\"]:\n",
    "        for m, d in release_info[t].iteritems():\n",
    "            if d < start_date_str:\n",
    "                continue\n",
    "            if d not in d2v or compare_ver(m, d2v[d]) > 0:\n",
    "                d2v[d] = m\n",
    "    last_ver = d2v[start_date_str]\n",
    "    # Fill in all the gaps:\n",
    "    for dt in (start_date + timedelta(n) for n in range(day_count)):\n",
    "        d = _datetime.strftime(dt, \"%Y-%m-%d\")\n",
    "        if d in d2v:\n",
    "            # Don't replace a higher version with a new release of an old\n",
    "            # version (probably an ESR release)\n",
    "            if compare_ver(d2v[d], last_ver) < 0:\n",
    "                d2v[d] = last_ver\n",
    "            else:\n",
    "                last_ver = d2v[d]\n",
    "        else:\n",
    "            d2v[d] = last_ver\n",
    "    return d2v\n",
    "\n",
    "def fetch_json(uri):\n",
    "    \"\"\" Perform an HTTP GET on the given uri, return the results as json. If\n",
    "    there is an error fetching the data, raise it.\n",
    "    \"\"\"\n",
    "    data = requests.get(uri)\n",
    "    # Raise an exception if the fetch failed.\n",
    "    data.raise_for_status()\n",
    "    return data.json()\n",
    "\n",
    "def compare_ver(a, b):\n",
    "    \"\"\" Logically compare two Firefox version strings. Split the string into\n",
    "    pieces, and compare each piece numerically.\n",
    "\n",
    "    Returns -1, 0, or 1 depending on whether a is less than, equal to, or\n",
    "    greater than b.\n",
    "    \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "\n",
    "    ap = [int(p) for p in a.split(\".\")]\n",
    "    bp = [int(p) for p in b.split(\".\")]\n",
    "    lap = len(ap)\n",
    "    lbp = len(bp)\n",
    "\n",
    "    # min # of pieces\n",
    "    mp = lap\n",
    "    if lbp < mp:\n",
    "        mp = lbp\n",
    "\n",
    "    for i in range(mp):\n",
    "        if ap[i] < bp[i]:\n",
    "            return -1\n",
    "        if ap[i] > bp[i]:\n",
    "            return 1\n",
    "\n",
    "    if lap > lbp:\n",
    "        # a has more pieces, common pieces are the same, a is greater\n",
    "        return 1\n",
    "\n",
    "    if lap < lbp:\n",
    "        # a has fewer pieces, common pieces are the same, b is greater\n",
    "        return -1\n",
    "\n",
    "    # They are exactly the same.\n",
    "    return 0\n",
    "\n",
    "def get_release_info():\n",
    "    \"\"\" Fetch information about Firefox release dates. Returns an object\n",
    "    containing two sections:\n",
    "\n",
    "    'major' - contains major releases (i.e. 41.0)\n",
    "    'minor' - contains stability releases (i.e. 41.0.1)\n",
    "    \"\"\"\n",
    "    major_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_major_releases.json\")\n",
    "    if major_info is None:\n",
    "        raise Exception(\"Failed to fetch major version info\")\n",
    "    minor_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_stability_releases.json\")\n",
    "    if minor_info is None:\n",
    "        raise Exception(\"Failed to fetch minor version info\")\n",
    "    return {\"major\": major_info, \"minor\": minor_info}\n",
    "\n",
    "def daynum_to_date(daynum):\n",
    "    \"\"\" Convert a number of days to a date. If it's out of range, default to a max date.\n",
    "    :param daynum: A number of days since Jan 1, 1970\n",
    "    \"\"\"\n",
    "    if daynum is None:\n",
    "        return None\n",
    "    if daynum < 0:\n",
    "        return None\n",
    "    daycount = int(daynum)\n",
    "    if daycount > 1000000:\n",
    "        # Some time in the 48th century, clearly bogus.\n",
    "        daycount = 1000000\n",
    "    return date(1970, 1, 1) + timedelta(daycount)\n",
    "\n",
    "def sane_date(d):\n",
    "    \"\"\" Check if the given date looks like a legitimate time on which activity\n",
    "    could have happened.\n",
    "    \"\"\"\n",
    "    if d is None:\n",
    "        return False\n",
    "    return d > date(2000, 1, 1) and d < _datetime.utcnow().date() + timedelta(2)\n",
    "\n",
    "def is_funnelcake(distro):\n",
    "    \"\"\" Check if a given distribution_id appears to be a funnelcake build.\"\"\"\n",
    "    if distro is None:\n",
    "        return False\n",
    "    return funnelcake_pattern.match(distro) is not None\n",
    "\n",
    "top_countries = set([\"US\", \"DE\", \"FR\", \"RU\", \"BR\", \"IN\", \"PL\", \"ID\", \"GB\", \"CN\",\n",
    "                  \"IT\", \"JP\", \"CA\", \"ES\", \"UA\", \"MX\", \"AU\", \"VN\", \"EG\", \"AR\",\n",
    "                  \"PH\", \"NL\", \"IR\", \"CZ\", \"HU\", \"TR\", \"RO\", \"GR\", \"AT\", \"CH\"])\n",
    "\n",
    "def top_country(country):\n",
    "    global top_countries\n",
    "    if(country in top_countries):\n",
    "        return country\n",
    "    return \"ROW\"\n",
    "\n",
    "def most_recent_sunday(d):\n",
    "    \"\"\" Get the date corresponding to the Sunday on or before the given date.\"\"\"\n",
    "    if d is None:\n",
    "        return None\n",
    "    weekday = d.weekday()\n",
    "    if weekday == 6:\n",
    "        return d\n",
    "    return d - timedelta(weekday + 1)\n",
    "\n",
    "def get_week_num(creation, today):\n",
    "    if creation is None or today is None:\n",
    "        return None\n",
    "\n",
    "    diff = (today.date() - creation).days\n",
    "    if diff < 0:\n",
    "        # Creation date is in the future. Bad data :(\n",
    "        return -1\n",
    "    # The initial week is week zero.\n",
    "    return int(diff / 7)\n",
    "\n",
    "# The number of seconds in a single hour, casted to float, so we get the fractional part\n",
    "# when converting.\n",
    "SECONDS_IN_HOUR = float(60 * 60)\n",
    "\n",
    "def convert(d2v, week_start, datum):\n",
    "    out = {\"good\": False}\n",
    "\n",
    "    pcd = daynum_to_date(datum.profile_creation_date)\n",
    "    if not sane_date(pcd):\n",
    "        return out\n",
    "\n",
    "    pcd_formatted = _datetime.strftime(pcd, \"%Y-%m-%d\")\n",
    "\n",
    "    out[\"client_id\"] = datum.client_id\n",
    "    channel = datum.normalized_channel\n",
    "    out[\"is_funnelcake\"] = is_funnelcake(datum.distribution_id)\n",
    "    if out[\"is_funnelcake\"]:\n",
    "        channel = \"{}-cck-{}\".format(datum.normalized_channel, datum.distribution_id)\n",
    "    out[\"channel\"] = channel\n",
    "    out[\"geo\"] = top_country(datum.country)\n",
    "    out[\"acquisition_period\"] = most_recent_sunday(pcd)\n",
    "    out[\"start_version\"] = get_effective_version(d2v, channel, pcd_formatted)\n",
    "\n",
    "    deviceCount = 0\n",
    "    if datum.sync_count_desktop is not None:\n",
    "        deviceCount += datum.sync_count_desktop\n",
    "    if datum.sync_count_mobile is not None:\n",
    "        deviceCount += datum.sync_count_mobile\n",
    "            \n",
    "    if deviceCount > 1:\n",
    "        out[\"sync_usage\"] = \"multiple\"\n",
    "    elif deviceCount == 1:\n",
    "        out[\"sync_usage\"] = \"single\"\n",
    "    elif datum.sync_configured is not None:\n",
    "        if datum.sync_configured:\n",
    "            out[\"sync_usage\"] = \"single\"\n",
    "        else:\n",
    "            out[\"sync_usage\"] = \"no\"\n",
    "    # Else we don't set sync_usage at all, and use a default value later.\n",
    "    \n",
    "    out[\"current_version\"] = datum.version\n",
    "    \n",
    "    # The usage time is in seconds, but we really need hours.\n",
    "    # Because we filter out broken subsession_lengths, we could end up with clients with no\n",
    "    # usage hours.\n",
    "    out[\"usage_hours\"] = (datum.usage_seconds / SECONDS_IN_HOUR) if datum.usage_seconds is not None else 0.0\n",
    "    out[\"squared_usage_hours\"] = out[\"usage_hours\"] ** 2\n",
    "    \n",
    "    # Incoming subsession_start_date looks like \"2016-02-22T00:00:00.0-04:00\"\n",
    "    client_date = None\n",
    "    if datum.subsession_start_date is not None:\n",
    "        try:\n",
    "            client_date = _datetime.strptime(datum.subsession_start_date[0:10], \"%Y-%m-%d\")\n",
    "        except ValueError as e1:\n",
    "            # Bogus format\n",
    "            pass\n",
    "        except TypeError as e2:\n",
    "            # String contains null bytes or other weirdness. Example:\n",
    "            # TypeError: must be string without null bytes, not unicode\n",
    "            pass\n",
    "    if client_date is None:\n",
    "        # Fall back to submission date\n",
    "        client_date = _datetime.strptime(datum.submission_date_s3, \"%Y%m%d\")\n",
    "    out[\"current_week\"] = get_week_num(pcd, client_date)\n",
    "    out[\"is_active\"] = \"yes\"\n",
    "    if client_date is not None:\n",
    "        try:\n",
    "            if _datetime.strftime(client_date, \"%Y%m%d\") < week_start:\n",
    "                out[\"is_active\"] = \"no\"\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "    out[\"good\"] = True\n",
    "    return out\n",
    "\n",
    "def csv(f):\n",
    "    return \",\".join([unicode(a) for a in f])\n",
    "\n",
    "# Build the \"effective version\" cache:\n",
    "d2v = make_d2v(get_release_info())\n",
    "\n",
    "def get_churn_filename(week_start, week_end):\n",
    "    return \"churn-{}-{}.by_activity.csv.gz\".format(week_start, week_end)\n",
    "\n",
    "def get_churn_filepath():\n",
    "    return \"amiyaguchi/churn\"\n",
    "\n",
    "def fmt(d, date_format=\"%Y%m%d\"):\n",
    "    return _datetime.strftime(d, date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the aggregates\n",
    "\n",
    "Run the aggregation code, detecting files that are missing.\n",
    "\n",
    "The fields we want in the output are:\n",
    " - channel (appUpdateChannel)\n",
    " - geo (bucketed into top 30 countries + \"rest of world\")\n",
    " - is_funnelcake (contains \"-cck-\"?)\n",
    " - acquisition_period (cohort_week)\n",
    " - start_version (effective version on profile creation date)\n",
    " - sync_usage (\"no\", \"single\" or \"multiple\" devices)\n",
    " - current_version (current appVersion)\n",
    " - current_week (week)\n",
    " - is_active (were the client_ids active this week or not)\n",
    " - n_profiles (count of matching client_ids)\n",
    " - usage_hours (sum of the per-client subsession lengths, clamped in the [0, MAX_SUBSESSION_LENGTH] range)\n",
    " - sum_squared_usage_hours (the sum of squares of the usage hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "\n",
    "MAX_SUBSESSION_LENGTH = 60 * 60 * 48 # 48 hours in seconds.\n",
    "bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "\n",
    "record_columns = [\n",
    "    'channel',\n",
    "    'geo',\n",
    "    'is_funnelcake',\n",
    "    'acquisition_period',\n",
    "    'start_version',\n",
    "    'sync_usage',\n",
    "    'current_version',\n",
    "    'current_week',\n",
    "    'is_active',\n",
    "    'n_profiles',\n",
    "    'usage_hours',\n",
    "    'sum_squared_usage_hours'\n",
    "]\n",
    "\n",
    "\n",
    "def get_newest_per_client(df):\n",
    "    windowSpec = Window.partitionBy(df['client_id']).orderBy(df['timestamp'].desc())\n",
    "    # Note: use 'rowNumber' instead of 'row_number' with Spark < v1.6\n",
    "    rownum_by_timestamp = (func.row_number().over(windowSpec))\n",
    "    selectable_by_client = df.select(\n",
    "        df['client_id'],\n",
    "        df['channel'],\n",
    "        df['normalized_channel'],\n",
    "        df['country'],\n",
    "        df['profile_creation_date'],\n",
    "        df['subsession_start_date'],\n",
    "        df['submission_date_s3'],\n",
    "        df['sync_configured'],\n",
    "        df['sync_count_desktop'],\n",
    "        df['sync_count_mobile'],\n",
    "        df['distribution_id'],\n",
    "        df['version'],\n",
    "        df['timestamp'],\n",
    "        rownum_by_timestamp.alias('row_number')\n",
    "    )\n",
    "    return selectable_by_client.filter(selectable_by_client['row_number'] == 1)\n",
    "\n",
    "\n",
    "def upload_to_s3(records, churn_outfile):\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = S3Transfer(client)\n",
    "\n",
    "    print \"{}: Writing output to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "\n",
    "    # Write the file out as gzipped csv\n",
    "    with gzip.open(churn_outfile, 'wb') as fout:\n",
    "        fout.write(\",\".join(record_columns) + \"\\n\")\n",
    "        print \"{}: Wrote header to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "        for r in records:\n",
    "            try:\n",
    "                fout.write(csv(r))\n",
    "                fout.write(\"\\n\")\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(\"{}: Error writing line: {} // {}\"\n",
    "                      .format(_datetime.utcnow(), e, r))\n",
    "        print \"{}: finished writing lines\".format(_datetime.utcnow())\n",
    "\n",
    "    # Now upload it to S3:\n",
    "    churn_s3 = \"{}/{}\".format(get_churn_filepath(), churn_outfile)\n",
    "    transfer.upload_file(churn_outfile, bucket, churn_s3,\n",
    "                         extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "\n",
    "    # Also upload it to the dashboard:\n",
    "    # Update the dashboard file\n",
    "    dash_bucket = \"net-mozaws-prod-metrics-data\"\n",
    "    dash_s3_name = \"telemetry-churn/{}\".format(churn_outfile)\n",
    "    transfer.upload_file(churn_outfile, dash_bucket, dash_s3_name,\n",
    "                         extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "\n",
    "    \n",
    "def compute_churn_week(df, week_start):\n",
    "    \"\"\"Compute the churn data for this week. Note that it takes 10 days\n",
    "    from the end of this period for all the activity to arrive. This data\n",
    "    should be from Sunday through Saturday.\n",
    "    \n",
    "    df: DataFrame of the dataset relevant to computing the churn\n",
    "    week_start: datestring of this time period\"\"\"\n",
    "    \n",
    "    week_start_date = _datetime.strptime(week_start, \"%Y%m%d\")\n",
    "    week_end_date = week_start_date + timedelta(6)\n",
    "    week_start = fmt(week_start_date)\n",
    "    week_end = fmt(week_end_date)\n",
    "    \n",
    "    # Verify that the start date is a Sunday\n",
    "    if week_start_date.weekday() != 6:\n",
    "        print(\"Week start date {} is not a Sunday\".format(week_start))\n",
    "        return\n",
    "    \n",
    "    # If the data for this week can still be coming, don't try to compute the churn.\n",
    "    week_end_slop = fmt(week_end_date + timedelta(10))\n",
    "    today = fmt(_datetime.utcnow())\n",
    "    if week_end_slop >= today:\n",
    "        print(\"Skipping week of {} to {} - Data is still arriving until {}.\"\n",
    "              .format(week_start, week_end, week_end_slop))\n",
    "        return\n",
    "    \n",
    "    print(\"Starting week from {} to {} at {}\"\n",
    "          .format(week_start, week_end, _datetime.utcnow()))\n",
    "    # the subsession_start_date field has a different form than submission_date_s3,\n",
    "    # so needs to be formatted with hyphens.\n",
    "    week_end_excl = fmt(week_end_date + timedelta(1), date_format=\"%Y-%m-%d\")\n",
    "    week_start_hyphenated = fmt(week_start_date, date_format=\"%Y-%m-%d\")\n",
    "\n",
    "    current_week = (\n",
    "        df.filter(df['submission_date_s3'] >= week_start)\n",
    "          .filter(df['submission_date_s3'] <= week_end_slop)\n",
    "          .filter(df['subsession_start_date'] >= week_start_hyphenated)\n",
    "          .filter(df['subsession_start_date'] < week_end_excl)\n",
    "    )\n",
    "    newest_per_client = get_newest_per_client(current_week)\n",
    "\n",
    "    # Clamp broken subsession values in the [0, MAX_SUBSESSION_LENGTH] range.\n",
    "    clamped_subsession = (\n",
    "        current_week.select(current_week['client_id'],\n",
    "            func.when(current_week['subsession_length'] > MAX_SUBSESSION_LENGTH, MAX_SUBSESSION_LENGTH)\n",
    "                .otherwise(func.when(current_week['subsession_length'] < 0, 0)\n",
    "                    .otherwise(current_week['subsession_length']))\n",
    "                .alias('subsession_length'))\n",
    "    )\n",
    "\n",
    "    # Compute the overall usage time for each client by summing the subsession lengths.\n",
    "    grouped_usage_time = (\n",
    "        clamped_subsession\n",
    "            .groupby('client_id')\n",
    "            .sum('subsession_length')\n",
    "            .withColumnRenamed('sum(subsession_length)', 'usage_seconds')\n",
    "    )\n",
    "\n",
    "    # Append this column to the original data frame.\n",
    "    newest_with_usage = newest_per_client.join(grouped_usage_time, 'client_id', 'inner')\n",
    "        \n",
    "    converted = newest_with_usage.rdd.map(lambda x: convert(d2v, week_start, x))\n",
    "\n",
    "    # Don't bother to filter out non-good records - they will appear \n",
    "    # as 'unknown' in the output.\n",
    "    countable = converted.map(lambda x: ((\n",
    "                x.get('channel', 'unknown'),\n",
    "                x.get('geo', 'unknown'),\n",
    "                \"yes\" if x.get('is_funnelcake', False) else \"no\",\n",
    "                _datetime.strftime(x.get('acquisition_period', date(2000, 1, 1)), \"%Y-%m-%d\"),\n",
    "                x.get('start_version', 'unknown'),\n",
    "                x.get('sync_usage', 'unknown'),\n",
    "                x.get('current_version', 'unknown'),\n",
    "                x.get('current_week', 'unknown'),\n",
    "                x.get('is_active', 'unknown')), (1, x.get('usage_hours', 0), x.get('squared_usage_hours', 0))))\n",
    "\n",
    "    def reduce_func(x, y):\n",
    "        return (x[0] + y[0], # Sum active users\n",
    "                x[1] + y[1], # Sum usage_hours\n",
    "                x[2] + y[2]) # Sum squared_usage_hours\n",
    "    aggregated = countable.reduceByKey(reduce_func)\n",
    "    \n",
    "    records_df = aggregated.map(lambda x: x[0] + x[1]).toDF(record_columns)\n",
    "\n",
    "    churn_outfile = get_churn_filename(week_start, week_end)\n",
    "    # upload_to_s3(records_df.rdd.collect(), churn_outfile)\n",
    "\n",
    "    \n",
    "    # Write to s3 as parquet, file size is on the order of 40MB\n",
    "    parquet_s3_path = (\"s3://{}/{}/v1/week_start={}\"\n",
    "                       .format(bucket, get_churn_filepath(), week_start))\n",
    "    print \"{}: Writing output as parquet to {}\".format(_datetime.utcnow(), parquet_s3_path)\n",
    "    records_df.repartition(1).write.parquet(parquet_s3_path, mode=\"overwrite\")\n",
    "\n",
    "    print(\"Finished week from {} to {} at {}\"\n",
    "          .format(week_start, week_end, _datetime.utcnow()))\n",
    "\n",
    "    \n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)//7):\n",
    "        yield (start_date + timedelta(n*7)).strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def backfill(df, start_date_yyyymmdd):\n",
    "    \"\"\" Import data from a start date to an end date\"\"\"\n",
    "    start_date = most_recent_sunday(_datetime.strptime(start_date_yyyymmdd, \"%Y%m%d\"))\n",
    "    end_date = _datetime.utcnow() - timedelta(1) # yesterday\n",
    "    for d in daterange(start_date, end_date):\n",
    "        try:\n",
    "            compute_churn_week(df, d)\n",
    "        except Exception as e:\n",
    "            log.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "env_date = environ.get('date', None)\n",
    "if not env_date:\n",
    "    raise ValueError(\"date not provided\")\n",
    "\n",
    "# 10 days of slack for incoming data + the 7 days used in churn computation\n",
    "week_start_date = most_recent_sunday(_datetime.strptime(env_date, \"%Y%m%d\") - timedelta(17))\n",
    "compute_churn_week(dataset, fmt(week_start_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run manually with an arbitratry Sunday chosen from the last few months\n",
    "# 20151101 is world start\n",
    "# compute_churn_week(dataset, '20160904')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
